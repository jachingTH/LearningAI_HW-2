{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9ff294-7d53-4404-bf73-ba0e5a1e4a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299407214933552\n",
      "=== epoch:1, train acc:0.146, test acc:0.124 ===\n",
      "train loss:2.2969977862206115\n",
      "train loss:2.2931071548476156\n",
      "train loss:2.2857982152383887\n",
      "train loss:2.2759235835891043\n",
      "train loss:2.264971431427963\n",
      "train loss:2.251932988608492\n",
      "train loss:2.2452031150134673\n",
      "train loss:2.1932820488609477\n",
      "train loss:2.186750310674302\n",
      "train loss:2.1494795653556555\n",
      "train loss:2.1249270352479908\n",
      "train loss:2.1021419326948263\n",
      "train loss:1.9998506749020057\n",
      "train loss:1.92256683251685\n",
      "train loss:1.8501187488653223\n",
      "train loss:1.82522770484278\n",
      "train loss:1.7722864922055013\n",
      "train loss:1.7244942603507412\n",
      "train loss:1.6111618269552728\n",
      "train loss:1.520515963356387\n",
      "train loss:1.4176982577702075\n",
      "train loss:1.3667842441546383\n",
      "train loss:1.2774103606019036\n",
      "train loss:1.1811714304246217\n",
      "train loss:1.0709867742952206\n",
      "train loss:1.0520869841538727\n",
      "train loss:1.064077949796921\n",
      "train loss:1.0170613097552486\n",
      "train loss:0.7693164844262028\n",
      "train loss:0.9012920587385557\n",
      "train loss:0.8497308981153701\n",
      "train loss:0.7484318446073109\n",
      "train loss:0.6025114447721279\n",
      "train loss:0.8398397437164549\n",
      "train loss:0.5105712845005734\n",
      "train loss:0.6426885733697685\n",
      "train loss:0.7158335472286411\n",
      "train loss:0.6196899186625788\n",
      "train loss:0.6771792613409358\n",
      "train loss:0.6970018362670003\n",
      "train loss:0.5320931903922441\n",
      "train loss:0.5618117538152904\n",
      "train loss:0.5645272823872828\n",
      "train loss:0.5769440045077945\n",
      "train loss:0.5388432977491844\n",
      "train loss:0.5993888376861176\n",
      "train loss:0.39312299847449644\n",
      "train loss:0.7166113984217408\n",
      "train loss:0.45048758815255086\n",
      "train loss:0.5558813592366062\n",
      "=== epoch:2, train acc:0.823, test acc:0.778 ===\n",
      "train loss:0.5239260200893425\n",
      "train loss:0.49763344304260726\n",
      "train loss:0.4985696934974726\n",
      "train loss:0.5030542887673941\n",
      "train loss:0.4982549425143231\n",
      "train loss:0.47435999323046646\n",
      "train loss:0.3243215215815147\n",
      "train loss:0.36523773723950176\n",
      "train loss:0.4295556761958522\n",
      "train loss:0.5211570173776581\n",
      "train loss:0.5171852787776341\n",
      "train loss:0.5051865831158193\n",
      "train loss:0.5036581109693435\n",
      "train loss:0.4359082203829888\n",
      "train loss:0.44514517401884246\n",
      "train loss:0.5133382699395697\n",
      "train loss:0.34163022457341363\n",
      "train loss:0.37218930501451125\n",
      "train loss:0.4847755840771056\n",
      "train loss:0.4473755736176862\n",
      "train loss:0.40692074953752233\n",
      "train loss:0.291238415370796\n",
      "train loss:0.35028419957393353\n",
      "train loss:0.4947335309230325\n",
      "train loss:0.40060603278846263\n",
      "train loss:0.3823482969817815\n",
      "train loss:0.3711621241326533\n",
      "train loss:0.29084186414677127\n",
      "train loss:0.35530529550118223\n",
      "train loss:0.2665104904990426\n",
      "train loss:0.3964622275543197\n",
      "train loss:0.37322991456917753\n",
      "train loss:0.2999882895480089\n",
      "train loss:0.4232484164453271\n",
      "train loss:0.3900623670971096\n",
      "train loss:0.18674946730443515\n",
      "train loss:0.2572468794992665\n",
      "train loss:0.31787399584324105\n",
      "train loss:0.36042481277728194\n",
      "train loss:0.38019909774964616\n",
      "train loss:0.38636694819610684\n",
      "train loss:0.23596855935919467\n",
      "train loss:0.5337308687765391\n",
      "train loss:0.37622880619206284\n",
      "train loss:0.32653445019793587\n",
      "train loss:0.3420822504736025\n",
      "train loss:0.4028573166983334\n",
      "train loss:0.3095806941890708\n",
      "train loss:0.2820146567444847\n",
      "train loss:0.43401692082879917\n",
      "=== epoch:3, train acc:0.866, test acc:0.857 ===\n",
      "train loss:0.27165118195143567\n",
      "train loss:0.3660045550384463\n",
      "train loss:0.2076477105757766\n",
      "train loss:0.15444633150280845\n",
      "train loss:0.1956431586610228\n",
      "train loss:0.4014607203958503\n",
      "train loss:0.3757009271025333\n",
      "train loss:0.40795217146975044\n",
      "train loss:0.4097488296847109\n",
      "train loss:0.41722180399436687\n",
      "train loss:0.22432433187453252\n",
      "train loss:0.2635217328441278\n",
      "train loss:0.2388276225647851\n",
      "train loss:0.30473258702300643\n",
      "train loss:0.31323032448261484\n",
      "train loss:0.3783348249782577\n",
      "train loss:0.4096573163342447\n",
      "train loss:0.3416290333281376\n",
      "train loss:0.24635758807367786\n",
      "train loss:0.20377012828305727\n",
      "train loss:0.3081255843740937\n",
      "train loss:0.24360798854227425\n",
      "train loss:0.3510680764822021\n",
      "train loss:0.2627632327436178\n",
      "train loss:0.20227098092540138\n",
      "train loss:0.22117355212662304\n",
      "train loss:0.18363777994603137\n",
      "train loss:0.4113196567482803\n",
      "train loss:0.25974215622315433\n",
      "train loss:0.12144760026158531\n",
      "train loss:0.29976360435970817\n",
      "train loss:0.3188527331365213\n",
      "train loss:0.29018533864414897\n",
      "train loss:0.17946106986957078\n",
      "train loss:0.2912695897782303\n",
      "train loss:0.2279668886806486\n",
      "train loss:0.24373814585739528\n",
      "train loss:0.15136618909561295\n",
      "train loss:0.179575501302057\n",
      "train loss:0.2063302612251508\n",
      "train loss:0.2931565058381464\n",
      "train loss:0.3997347822953654\n",
      "train loss:0.23675287145920507\n",
      "train loss:0.2729795263892702\n",
      "train loss:0.37511642750455804\n",
      "train loss:0.4099570003296275\n",
      "train loss:0.24322114106398243\n",
      "train loss:0.2103917019214892\n",
      "train loss:0.16732909835510323\n",
      "train loss:0.24018926518633216\n",
      "=== epoch:4, train acc:0.9, test acc:0.884 ===\n",
      "train loss:0.216534181061755\n",
      "train loss:0.376485111685373\n",
      "train loss:0.2868572407936368\n",
      "train loss:0.2710523047324945\n",
      "train loss:0.3456165081542569\n",
      "train loss:0.23828471376087917\n",
      "train loss:0.23966159765066292\n",
      "train loss:0.1997788583710961\n",
      "train loss:0.20246450419648046\n",
      "train loss:0.3135757023279073\n",
      "train loss:0.1292058003962826\n",
      "train loss:0.2034400834673914\n",
      "train loss:0.2210324205413762\n",
      "train loss:0.5763178420300226\n",
      "train loss:0.4155603419102584\n",
      "train loss:0.25542035338635327\n",
      "train loss:0.17686081644898077\n",
      "train loss:0.1610171039088189\n",
      "train loss:0.24888544957186537\n",
      "train loss:0.43834225831788487\n",
      "train loss:0.12559088054108247\n",
      "train loss:0.37410616802377233\n",
      "train loss:0.2931580847760043\n",
      "train loss:0.3351710089459557\n",
      "train loss:0.19771464676511546\n",
      "train loss:0.2601414722220937\n",
      "train loss:0.2580031031467921\n",
      "train loss:0.26425261419559304\n",
      "train loss:0.26325889658212054\n",
      "train loss:0.18260081192365102\n",
      "train loss:0.16252546159984071\n",
      "train loss:0.2034737327679622\n",
      "train loss:0.20595859235433353\n",
      "train loss:0.15595749275947068\n",
      "train loss:0.320244744048411\n",
      "train loss:0.16712664160032595\n",
      "train loss:0.5316874478787001\n",
      "train loss:0.2400237532463652\n",
      "train loss:0.17979786205141168\n",
      "train loss:0.22350945522940294\n",
      "train loss:0.1644568466155756\n",
      "train loss:0.2026597969932405\n",
      "train loss:0.22997465934499306\n",
      "train loss:0.32269374022981934\n",
      "train loss:0.29778533502471594\n",
      "train loss:0.17579587178475264\n",
      "train loss:0.13878957018952431\n",
      "train loss:0.17166341966362353\n",
      "train loss:0.14335588590909146\n",
      "train loss:0.35190097560882405\n",
      "=== epoch:5, train acc:0.935, test acc:0.901 ===\n",
      "train loss:0.2134866476766489\n",
      "train loss:0.2697035455275189\n",
      "train loss:0.15390871938555586\n",
      "train loss:0.13673745922112612\n",
      "train loss:0.2602917383751249\n",
      "train loss:0.3650233518637159\n",
      "train loss:0.231056488607797\n",
      "train loss:0.18049784741181502\n",
      "train loss:0.17973917891550265\n",
      "train loss:0.18326434333688504\n",
      "train loss:0.1588180103296255\n",
      "train loss:0.3761835407669356\n",
      "train loss:0.2285894154485034\n",
      "train loss:0.19393065539049517\n",
      "train loss:0.28075956078162656\n",
      "train loss:0.20343334512499112\n",
      "train loss:0.18170752841756585\n",
      "train loss:0.17433357171627242\n",
      "train loss:0.1698552244720376\n",
      "train loss:0.2477390228053829\n",
      "train loss:0.34314073453741295\n",
      "train loss:0.22452903073282954\n",
      "train loss:0.14722348125750176\n",
      "train loss:0.20297717889266767\n",
      "train loss:0.22250278765684633\n",
      "train loss:0.23015982172280336\n",
      "train loss:0.09726249000117854\n",
      "train loss:0.22419982341087452\n",
      "train loss:0.1458626124901707\n",
      "train loss:0.1878296362002355\n",
      "train loss:0.1678097680035518\n",
      "train loss:0.27580797197528295\n",
      "train loss:0.1850109721465217\n",
      "train loss:0.21229186466126823\n",
      "train loss:0.1505897007566384\n",
      "train loss:0.07684231054818186\n",
      "train loss:0.1852347709274809\n",
      "train loss:0.11985187162727934\n",
      "train loss:0.19360863590549954\n",
      "train loss:0.15973843889926406\n",
      "train loss:0.2075950015107606\n",
      "train loss:0.12787306323860556\n",
      "train loss:0.12139649355352372\n",
      "train loss:0.10496434900973313\n",
      "train loss:0.2331939361303397\n",
      "train loss:0.21516132560095003\n",
      "train loss:0.27295250966308915\n",
      "train loss:0.12286527065818337\n",
      "train loss:0.1916653569686897\n",
      "train loss:0.188384859500075\n",
      "=== epoch:6, train acc:0.923, test acc:0.921 ===\n",
      "train loss:0.24835710110594453\n",
      "train loss:0.22190225414314926\n",
      "train loss:0.15876167071309605\n",
      "train loss:0.09628124907799687\n",
      "train loss:0.12152088886031637\n",
      "train loss:0.29760530859548545\n",
      "train loss:0.125195483621108\n",
      "train loss:0.15199474673995986\n",
      "train loss:0.2171580144839228\n",
      "train loss:0.06725529457386195\n",
      "train loss:0.13319648078185928\n",
      "train loss:0.21472636664062258\n",
      "train loss:0.264492868717227\n",
      "train loss:0.15955123197168308\n",
      "train loss:0.23110890519575433\n",
      "train loss:0.11057807162545384\n",
      "train loss:0.2047611787360004\n",
      "train loss:0.10524561049780352\n",
      "train loss:0.09759739286726556\n",
      "train loss:0.09426896605169473\n",
      "train loss:0.129501369473728\n",
      "train loss:0.21636729147154823\n",
      "train loss:0.28468183131584224\n",
      "train loss:0.17252939820178384\n",
      "train loss:0.14403920697996214\n",
      "train loss:0.15782706200603847\n",
      "train loss:0.2975244815121303\n",
      "train loss:0.13255692523079624\n",
      "train loss:0.1700804696819013\n",
      "train loss:0.1566081961178885\n",
      "train loss:0.19750767611973255\n",
      "train loss:0.2221549715719199\n",
      "train loss:0.22076245016048918\n",
      "train loss:0.16968588440459398\n",
      "train loss:0.08650909208818797\n",
      "train loss:0.15337657521671308\n",
      "train loss:0.09429650986530864\n",
      "train loss:0.3563042391265601\n",
      "train loss:0.10849661870703725\n",
      "train loss:0.18153420978032755\n",
      "train loss:0.18993073233574628\n",
      "train loss:0.20296666575346842\n",
      "train loss:0.16285261087384653\n",
      "train loss:0.22539526530871354\n",
      "train loss:0.30675076475360763\n",
      "train loss:0.2039736994575805\n",
      "train loss:0.29259501214359246\n",
      "train loss:0.14789442884866333\n",
      "train loss:0.1340315243174808\n",
      "train loss:0.14562830499146007\n",
      "=== epoch:7, train acc:0.947, test acc:0.922 ===\n",
      "train loss:0.14850447537675865\n",
      "train loss:0.15772297733128146\n",
      "train loss:0.11974720378801555\n",
      "train loss:0.17702855661451763\n",
      "train loss:0.19671819385941636\n",
      "train loss:0.15572939252714646\n",
      "train loss:0.1777098853072799\n",
      "train loss:0.17299739582806087\n",
      "train loss:0.11232526577779399\n",
      "train loss:0.11819193795148224\n",
      "train loss:0.11357862814550308\n",
      "train loss:0.13185061955389427\n",
      "train loss:0.14195834768517096\n",
      "train loss:0.1527280539505537\n",
      "train loss:0.067182902775462\n",
      "train loss:0.12701002849145943\n",
      "train loss:0.17570276537429413\n",
      "train loss:0.06304810446996263\n",
      "train loss:0.2679513200177658\n",
      "train loss:0.17166071297109384\n",
      "train loss:0.06766556912848234\n",
      "train loss:0.07499256907460411\n",
      "train loss:0.23815236382078425\n",
      "train loss:0.15042241123038314\n",
      "train loss:0.08367821082178718\n",
      "train loss:0.14038481039528247\n",
      "train loss:0.07185483020050557\n",
      "train loss:0.12572190885027673\n",
      "train loss:0.10006038082949047\n",
      "train loss:0.06903894911925285\n",
      "train loss:0.11675345340922147\n",
      "train loss:0.15346566705396822\n",
      "train loss:0.10767993636895065\n",
      "train loss:0.08331732352609991\n",
      "train loss:0.1610052843183152\n",
      "train loss:0.17706672017803915\n",
      "train loss:0.20329746172595353\n",
      "train loss:0.1723925840253323\n",
      "train loss:0.1706978460687909\n",
      "train loss:0.13369067636675136\n",
      "train loss:0.1995349768718555\n",
      "train loss:0.16181696707413631\n",
      "train loss:0.14442296616984437\n",
      "train loss:0.0750423880780995\n",
      "train loss:0.10665050066108035\n",
      "train loss:0.08666512067649211\n",
      "train loss:0.10927606286654873\n",
      "train loss:0.16806717510825644\n",
      "train loss:0.08880146661439231\n",
      "train loss:0.07929740442157307\n",
      "=== epoch:8, train acc:0.958, test acc:0.934 ===\n",
      "train loss:0.06200881554456977\n",
      "train loss:0.09619520764722063\n",
      "train loss:0.07927144128303072\n",
      "train loss:0.08356463048379513\n",
      "train loss:0.08236501873850857\n",
      "train loss:0.07922733213859652\n",
      "train loss:0.13165133102252524\n",
      "train loss:0.11606692751429484\n",
      "train loss:0.0652557756631463\n",
      "train loss:0.0835298682969307\n",
      "train loss:0.06593792924375325\n",
      "train loss:0.1978701643943176\n",
      "train loss:0.06581199167364093\n",
      "train loss:0.25109438335391354\n",
      "train loss:0.130192574896259\n",
      "train loss:0.18676440423809898\n",
      "train loss:0.09439603593109934\n",
      "train loss:0.1377093516076685\n",
      "train loss:0.09953647009494862\n",
      "train loss:0.15362359793230074\n",
      "train loss:0.09066150662725254\n",
      "train loss:0.07305907351755819\n",
      "train loss:0.10075729664307415\n",
      "train loss:0.10379803765445322\n",
      "train loss:0.10687751602093158\n",
      "train loss:0.07304674564912923\n",
      "train loss:0.09970377216812279\n",
      "train loss:0.08878435740159145\n",
      "train loss:0.1293329195415806\n",
      "train loss:0.13823612692953435\n",
      "train loss:0.12620881338827\n",
      "train loss:0.07394258609154826\n",
      "train loss:0.13056548245233301\n",
      "train loss:0.052281213707892064\n",
      "train loss:0.14700421670606134\n",
      "train loss:0.10925058547495003\n",
      "train loss:0.1598060347460906\n",
      "train loss:0.09703588524636092\n",
      "train loss:0.10361683541384784\n",
      "train loss:0.08663465765875884\n",
      "train loss:0.13485343006883277\n",
      "train loss:0.06706024599855036\n",
      "train loss:0.10790781880021878\n",
      "train loss:0.04962922787238975\n",
      "train loss:0.10301017038364618\n",
      "train loss:0.09160659544041244\n",
      "train loss:0.14768231123807177\n",
      "train loss:0.08284536606134463\n",
      "train loss:0.060213930178946634\n",
      "train loss:0.04568818525675658\n",
      "=== epoch:9, train acc:0.971, test acc:0.934 ===\n",
      "train loss:0.06077611225298529\n",
      "train loss:0.07463951735798018\n",
      "train loss:0.09409467805895863\n",
      "train loss:0.07543175163290378\n",
      "train loss:0.13597941443130496\n",
      "train loss:0.20297907008298158\n",
      "train loss:0.07632891092274492\n",
      "train loss:0.14224615574040902\n",
      "train loss:0.04411938963556275\n",
      "train loss:0.09144669306840843\n",
      "train loss:0.1326747632175887\n",
      "train loss:0.1057767727273075\n",
      "train loss:0.1164482786741519\n",
      "train loss:0.13499648519558144\n",
      "train loss:0.13439872383450005\n",
      "train loss:0.1469394411526127\n",
      "train loss:0.08039918051968074\n",
      "train loss:0.17025808426749553\n",
      "train loss:0.042719862628469905\n",
      "train loss:0.12892722241925073\n",
      "train loss:0.07929601132482031\n",
      "train loss:0.08146967694521338\n",
      "train loss:0.08167721817390959\n",
      "train loss:0.057761556804171936\n",
      "train loss:0.08244763073765986\n",
      "train loss:0.11759164766903446\n",
      "train loss:0.11235606716548346\n",
      "train loss:0.10618702719666646\n",
      "train loss:0.05869955457305323\n",
      "train loss:0.15834816283105352\n",
      "train loss:0.06359127582436296\n",
      "train loss:0.13548408414266228\n",
      "train loss:0.0630520089442566\n",
      "train loss:0.13268082560990885\n",
      "train loss:0.05304962998330267\n",
      "train loss:0.05784642203282481\n",
      "train loss:0.07233273792633658\n",
      "train loss:0.0782480688272396\n",
      "train loss:0.14633506571857158\n",
      "train loss:0.038464789456064116\n",
      "train loss:0.06141630284458464\n",
      "train loss:0.03326283307236218\n",
      "train loss:0.04294839285436835\n",
      "train loss:0.1061265057761976\n",
      "train loss:0.21414873029946715\n",
      "train loss:0.13012682360412778\n",
      "train loss:0.06524834199186035\n",
      "train loss:0.1132650556785917\n",
      "train loss:0.08694421801591533\n",
      "train loss:0.1378805842390263\n",
      "=== epoch:10, train acc:0.962, test acc:0.934 ===\n",
      "train loss:0.08558384724425848\n",
      "train loss:0.11327452220194836\n",
      "train loss:0.10517618269325027\n",
      "train loss:0.05268324392700863\n",
      "train loss:0.08396148292684665\n",
      "train loss:0.09119690168424924\n",
      "train loss:0.0855965384797645\n",
      "train loss:0.14417740429769765\n",
      "train loss:0.06649580977854308\n",
      "train loss:0.05909332434204773\n",
      "train loss:0.2336546045192177\n",
      "train loss:0.10464389704125787\n",
      "train loss:0.14594576146914132\n",
      "train loss:0.06619763801189876\n",
      "train loss:0.18387075736504613\n",
      "train loss:0.15351711853840413\n",
      "train loss:0.10798198616470021\n",
      "train loss:0.07469676912546949\n",
      "train loss:0.08271100325634759\n",
      "train loss:0.03904559908267605\n",
      "train loss:0.05944316198164808\n",
      "train loss:0.06724656934035292\n",
      "train loss:0.11841898814537595\n",
      "train loss:0.04704993942020011\n",
      "train loss:0.07677656728907495\n",
      "train loss:0.10694927623407677\n",
      "train loss:0.06734513941031878\n",
      "train loss:0.07798850921608949\n",
      "train loss:0.1352432502459558\n",
      "train loss:0.07351286897292136\n",
      "train loss:0.0858262868695057\n",
      "train loss:0.11936541247162168\n",
      "train loss:0.04925374971105337\n",
      "train loss:0.05790452694323844\n",
      "train loss:0.04235603679489461\n",
      "train loss:0.08221852381342101\n",
      "train loss:0.06721168093035206\n",
      "train loss:0.13009680905501428\n",
      "train loss:0.0898902363521156\n",
      "train loss:0.07250929590096437\n",
      "train loss:0.0256681265861572\n",
      "train loss:0.08127517554553652\n",
      "train loss:0.061188095888875166\n",
      "train loss:0.1546659890383704\n",
      "train loss:0.08695592602632647\n",
      "train loss:0.06696930306165515\n",
      "train loss:0.08714641462972933\n",
      "train loss:0.08566289160693721\n",
      "train loss:0.1240124559180276\n",
      "train loss:0.09553218940204372\n",
      "=== epoch:11, train acc:0.964, test acc:0.943 ===\n",
      "train loss:0.05699359445346415\n",
      "train loss:0.03636931723257432\n",
      "train loss:0.065076809015423\n",
      "train loss:0.17693049332130187\n",
      "train loss:0.04378212805961908\n",
      "train loss:0.10833588334990171\n",
      "train loss:0.179569119900316\n",
      "train loss:0.04052235925973146\n",
      "train loss:0.06497982732482668\n",
      "train loss:0.10659772915616982\n",
      "train loss:0.029694993965550177\n",
      "train loss:0.04974155685680807\n",
      "train loss:0.10565364689275712\n",
      "train loss:0.050153162864457555\n",
      "train loss:0.07181696925158475\n",
      "train loss:0.1733071153582478\n",
      "train loss:0.04879462281435212\n",
      "train loss:0.11036704438852027\n",
      "train loss:0.05111239617770317\n",
      "train loss:0.04940029471424125\n",
      "train loss:0.07050494088778249\n",
      "train loss:0.05003399800934515\n",
      "train loss:0.13078361414175552\n",
      "train loss:0.07535413714757819\n",
      "train loss:0.07893988548079539\n",
      "train loss:0.05625433361379551\n",
      "train loss:0.08452644901366187\n",
      "train loss:0.06580583551650829\n",
      "train loss:0.0684397497171782\n",
      "train loss:0.03474359373506922\n",
      "train loss:0.07231996991257794\n",
      "train loss:0.07238477421276472\n",
      "train loss:0.06821488000387892\n",
      "train loss:0.04184571869201521\n",
      "train loss:0.10788006912513148\n",
      "train loss:0.07406427330431063\n",
      "train loss:0.10688982446886067\n",
      "train loss:0.02828996381704122\n",
      "train loss:0.030039014233776657\n",
      "train loss:0.09275558505018623\n",
      "train loss:0.10135886156905444\n",
      "train loss:0.06276867726214738\n",
      "train loss:0.035736973040871395\n",
      "train loss:0.035612337109101895\n",
      "train loss:0.03409208991681078\n",
      "train loss:0.05298270805479247\n",
      "train loss:0.04052646163371982\n",
      "train loss:0.05430159593028674\n",
      "train loss:0.06455905267430255\n",
      "train loss:0.022500597756536112\n",
      "=== epoch:12, train acc:0.98, test acc:0.946 ===\n",
      "train loss:0.10866866723098113\n",
      "train loss:0.02311713509035068\n",
      "train loss:0.06564155994038968\n",
      "train loss:0.04965787527091828\n",
      "train loss:0.030238427082110456\n",
      "train loss:0.07546105512784224\n",
      "train loss:0.07413500350548866\n",
      "train loss:0.09774687080863137\n",
      "train loss:0.02234990315455801\n",
      "train loss:0.04643929677703437\n",
      "train loss:0.10640663272348876\n",
      "train loss:0.05234425035236354\n",
      "train loss:0.05236336965566472\n",
      "train loss:0.01716812345706254\n",
      "train loss:0.07101280079292907\n",
      "train loss:0.052766885365371635\n",
      "train loss:0.1316872385131221\n",
      "train loss:0.10918992562438834\n",
      "train loss:0.06203463743644351\n",
      "train loss:0.08549178716722608\n",
      "train loss:0.07925047359805053\n",
      "train loss:0.08583395755314108\n",
      "train loss:0.08245928343618508\n",
      "train loss:0.04382870601201106\n",
      "train loss:0.0529158348724023\n",
      "train loss:0.050111691339967776\n",
      "train loss:0.038001508499833664\n",
      "train loss:0.05153468429204039\n",
      "train loss:0.043144582856918155\n",
      "train loss:0.07122615229491983\n",
      "train loss:0.09041633936536927\n",
      "train loss:0.056364565671007505\n",
      "train loss:0.044959669741459526\n",
      "train loss:0.04938244350516405\n",
      "train loss:0.0378249948654857\n",
      "train loss:0.12382566801424788\n",
      "train loss:0.0642257452734845\n",
      "train loss:0.032804183170325324\n",
      "train loss:0.05603433482505776\n",
      "train loss:0.016356997782748902\n",
      "train loss:0.16943354709942893\n",
      "train loss:0.09485978405275221\n",
      "train loss:0.03214280891535147\n",
      "train loss:0.0922131152661291\n",
      "train loss:0.035864571437944505\n",
      "train loss:0.055604585976376866\n",
      "train loss:0.05077968147745007\n",
      "train loss:0.031214739094706262\n",
      "train loss:0.02304055846577198\n",
      "train loss:0.041152819602052186\n",
      "=== epoch:13, train acc:0.977, test acc:0.948 ===\n",
      "train loss:0.09894848404068686\n",
      "train loss:0.028121046908130845\n",
      "train loss:0.016948668879799244\n",
      "train loss:0.03267422555065623\n",
      "train loss:0.15232062560422446\n",
      "train loss:0.049601189369150395\n",
      "train loss:0.029391532830653436\n",
      "train loss:0.034546579821466444\n",
      "train loss:0.03730402602565442\n",
      "train loss:0.09714104888268749\n",
      "train loss:0.05273815626010997\n",
      "train loss:0.022263791916039336\n",
      "train loss:0.07591867377285222\n",
      "train loss:0.03317501566458107\n",
      "train loss:0.03221297207034869\n",
      "train loss:0.0642627538620685\n",
      "train loss:0.05849234221965971\n",
      "train loss:0.04318073589109386\n",
      "train loss:0.059960545428601415\n",
      "train loss:0.04606721427429906\n",
      "train loss:0.04627370678327822\n",
      "train loss:0.03317224726550216\n",
      "train loss:0.02958381143085244\n",
      "train loss:0.11929041587550852\n",
      "train loss:0.06741078411244192\n",
      "train loss:0.033023916493679376\n",
      "train loss:0.06293352742451228\n",
      "train loss:0.051011248952642106\n",
      "train loss:0.08001847397868186\n",
      "train loss:0.07139441043765477\n",
      "train loss:0.05764947228812371\n",
      "train loss:0.021119464418082004\n",
      "train loss:0.03558671965370475\n",
      "train loss:0.033620499707438654\n",
      "train loss:0.035163796306157166\n",
      "train loss:0.017261559369343486\n",
      "train loss:0.09188815346157073\n",
      "train loss:0.023801482528121566\n",
      "train loss:0.020983624191158737\n",
      "train loss:0.07625674172440473\n",
      "train loss:0.03973213698981085\n",
      "train loss:0.04888014497690599\n",
      "train loss:0.028681421149314313\n",
      "train loss:0.038036389195397355\n",
      "train loss:0.04036391561791561\n",
      "train loss:0.038593777233259074\n",
      "train loss:0.07903468618925105\n",
      "train loss:0.035026449508534396\n",
      "train loss:0.07060885649394581\n",
      "train loss:0.027339709385558585\n",
      "=== epoch:14, train acc:0.988, test acc:0.954 ===\n",
      "train loss:0.04018859067533495\n",
      "train loss:0.059312933862775835\n",
      "train loss:0.026200683517510495\n",
      "train loss:0.03874965040338854\n",
      "train loss:0.03245308962212224\n",
      "train loss:0.02308290887598628\n",
      "train loss:0.04206872892652356\n",
      "train loss:0.0604118895093694\n",
      "train loss:0.037581736031872384\n",
      "train loss:0.03716878920566583\n",
      "train loss:0.029149024511093656\n",
      "train loss:0.07450744719304456\n",
      "train loss:0.03573571484506862\n",
      "train loss:0.06491265855351806\n",
      "train loss:0.03391304021945451\n",
      "train loss:0.07284732156628924\n",
      "train loss:0.015810994889358342\n",
      "train loss:0.035841513630224406\n",
      "train loss:0.028951972722219027\n",
      "train loss:0.04207929011540773\n",
      "train loss:0.016459009434368323\n",
      "train loss:0.010531177794957863\n",
      "train loss:0.05304788458501966\n",
      "train loss:0.03813762286925617\n",
      "train loss:0.010570488978226638\n",
      "train loss:0.0905263185495386\n",
      "train loss:0.09111549777782331\n",
      "train loss:0.015275751055171127\n",
      "train loss:0.03317900231934382\n",
      "train loss:0.029432691141896585\n",
      "train loss:0.058565834994274334\n",
      "train loss:0.08458650517554527\n",
      "train loss:0.016097393173423468\n",
      "train loss:0.02892934069197949\n",
      "train loss:0.05444164660910956\n",
      "train loss:0.021548873680533508\n",
      "train loss:0.028962272380631267\n",
      "train loss:0.06894449784930633\n",
      "train loss:0.023701411489562455\n",
      "train loss:0.024665255466425687\n",
      "train loss:0.02748690640796341\n",
      "train loss:0.03806847300183825\n",
      "train loss:0.02552978349344039\n",
      "train loss:0.08946904780246406\n",
      "train loss:0.040586020437256495\n",
      "train loss:0.034029164525864145\n",
      "train loss:0.1024965210309667\n",
      "train loss:0.06287673571074119\n",
      "train loss:0.053494524190433095\n",
      "train loss:0.06714592337628104\n",
      "=== epoch:15, train acc:0.976, test acc:0.951 ===\n",
      "train loss:0.05352212557463942\n",
      "train loss:0.03637745770122682\n",
      "train loss:0.028561352619316752\n",
      "train loss:0.04338311168728477\n",
      "train loss:0.019662843366646837\n",
      "train loss:0.034547536424494554\n",
      "train loss:0.05621756149206087\n",
      "train loss:0.09459674866667035\n",
      "train loss:0.022885312837083574\n",
      "train loss:0.04998415959819215\n",
      "train loss:0.025234181154306427\n",
      "train loss:0.05217337626044755\n",
      "train loss:0.056402013921717196\n",
      "train loss:0.05240332674925125\n",
      "train loss:0.07615116977692143\n",
      "train loss:0.03208438829635349\n",
      "train loss:0.023394696206642935\n",
      "train loss:0.018694730077408742\n",
      "train loss:0.0277561318163457\n",
      "train loss:0.09694359941702381\n",
      "train loss:0.012827241095147802\n",
      "train loss:0.013011828521411853\n",
      "train loss:0.03690806135767375\n",
      "train loss:0.048493177910764816\n",
      "train loss:0.03299082525633029\n",
      "train loss:0.03801211793328268\n",
      "train loss:0.015742056368787904\n",
      "train loss:0.06315614058757515\n",
      "train loss:0.08091833128053295\n",
      "train loss:0.015345978371214195\n",
      "train loss:0.04492137307564529\n",
      "train loss:0.017702524994944516\n",
      "train loss:0.021540714042152253\n",
      "train loss:0.01169574110424473\n",
      "train loss:0.04413070935291108\n",
      "train loss:0.044820532095861176\n",
      "train loss:0.03178967663294638\n",
      "train loss:0.052048416641421145\n",
      "train loss:0.05303519736887031\n",
      "train loss:0.027831090121624262\n",
      "train loss:0.06095702394686969\n",
      "train loss:0.0570155405209009\n",
      "train loss:0.0280028096739653\n",
      "train loss:0.05161235663285347\n",
      "train loss:0.020455559680060097\n",
      "train loss:0.03543062093700941\n",
      "train loss:0.1119753056566373\n",
      "train loss:0.01892635582462708\n",
      "train loss:0.008249779827366438\n",
      "train loss:0.03189071653084957\n",
      "=== epoch:16, train acc:0.985, test acc:0.957 ===\n",
      "train loss:0.052057834705827505\n",
      "train loss:0.033803340379256495\n",
      "train loss:0.04893658775839283\n",
      "train loss:0.014487979670395145\n",
      "train loss:0.02679918105884771\n",
      "train loss:0.027367313714287647\n",
      "train loss:0.06786462497908972\n",
      "train loss:0.053430085608701085\n",
      "train loss:0.027451668455460997\n",
      "train loss:0.032116513775605676\n",
      "train loss:0.03627604392543225\n",
      "train loss:0.02906313686474758\n",
      "train loss:0.023150327785550057\n",
      "train loss:0.04339887120186222\n",
      "train loss:0.00881503693836453\n",
      "train loss:0.025250297866740757\n",
      "train loss:0.030512557364298844\n",
      "train loss:0.017275211215505144\n",
      "train loss:0.047222732747857876\n",
      "train loss:0.02452414239162586\n",
      "train loss:0.06646473170858642\n",
      "train loss:0.019503228335885265\n",
      "train loss:0.022863054968937764\n",
      "train loss:0.024666285851056395\n",
      "train loss:0.031149173274530418\n",
      "train loss:0.06964358513435737\n",
      "train loss:0.040267190493136595\n",
      "train loss:0.021273258461234134\n",
      "train loss:0.012962097807462212\n",
      "train loss:0.01721769958502635\n",
      "train loss:0.046057386942925715\n",
      "train loss:0.028463697039477348\n",
      "train loss:0.027840012998242805\n",
      "train loss:0.03388488465623145\n",
      "train loss:0.010548576593904931\n",
      "train loss:0.02746782275643494\n",
      "train loss:0.042941476630098965\n",
      "train loss:0.03659647779974813\n",
      "train loss:0.025559587675427032\n",
      "train loss:0.017459287534968983\n",
      "train loss:0.02431006015048856\n",
      "train loss:0.08220629508245998\n",
      "train loss:0.04299406293340174\n",
      "train loss:0.017036356060580918\n",
      "train loss:0.06793343083226215\n",
      "train loss:0.03799725689681257\n",
      "train loss:0.06321520857451374\n",
      "train loss:0.027892623926068793\n",
      "train loss:0.029344703900684263\n",
      "train loss:0.03321595514085876\n",
      "=== epoch:17, train acc:0.981, test acc:0.952 ===\n",
      "train loss:0.04574712191909778\n",
      "train loss:0.025418005199958568\n",
      "train loss:0.01896570733942406\n",
      "train loss:0.03984572114559703\n",
      "train loss:0.035265037403964056\n",
      "train loss:0.016123882323922947\n",
      "train loss:0.008736555041350501\n",
      "train loss:0.016189485888883817\n",
      "train loss:0.02696283266150682\n",
      "train loss:0.015734953855221216\n",
      "train loss:0.0633804610141133\n",
      "train loss:0.02940322029561758\n",
      "train loss:0.017909934988786593\n",
      "train loss:0.012334182733262384\n",
      "train loss:0.012229906679019546\n",
      "train loss:0.01966212251494711\n",
      "train loss:0.015574307103235492\n",
      "train loss:0.02593711025784342\n",
      "train loss:0.014040065560076101\n",
      "train loss:0.0936932690632991\n",
      "train loss:0.019255333561699513\n",
      "train loss:0.0324424844295872\n",
      "train loss:0.08226821550912336\n",
      "train loss:0.06095708290232039\n",
      "train loss:0.021994661820103994\n",
      "train loss:0.009804093392807344\n",
      "train loss:0.03204955783490341\n",
      "train loss:0.036947276424659656\n",
      "train loss:0.016174804653027876\n",
      "train loss:0.021035862626341262\n",
      "train loss:0.030818803052151962\n",
      "train loss:0.035031057450257516\n",
      "train loss:0.03535308217467632\n",
      "train loss:0.013351004203145585\n",
      "train loss:0.019559253790358236\n",
      "train loss:0.029522286028928216\n",
      "train loss:0.033720108013689964\n",
      "train loss:0.02057933120429529\n",
      "train loss:0.015313081392523152\n",
      "train loss:0.030297359723554282\n",
      "train loss:0.013294523415484203\n",
      "train loss:0.01350454977729832\n",
      "train loss:0.04289013061395506\n",
      "train loss:0.024642010406847565\n",
      "train loss:0.013154952312173517\n",
      "train loss:0.016620557623450046\n",
      "train loss:0.012616472261275047\n",
      "train loss:0.032482746147099145\n",
      "train loss:0.05075871525230113\n",
      "train loss:0.008377156434512457\n",
      "=== epoch:18, train acc:0.99, test acc:0.953 ===\n",
      "train loss:0.01820335429841006\n",
      "train loss:0.07358861185562601\n",
      "train loss:0.04500773847193747\n",
      "train loss:0.009900299636622744\n",
      "train loss:0.02217644086140115\n",
      "train loss:0.025783028476237993\n",
      "train loss:0.035668511754076225\n",
      "train loss:0.010973206889035325\n",
      "train loss:0.019991600752387328\n",
      "train loss:0.014971564996284768\n",
      "train loss:0.02555790975184477\n",
      "train loss:0.014864659805274343\n",
      "train loss:0.03547181847083356\n",
      "train loss:0.019631596668325507\n",
      "train loss:0.007764055165929143\n",
      "train loss:0.04463254747015873\n",
      "train loss:0.013329000338749683\n",
      "train loss:0.03527614040271809\n",
      "train loss:0.007101396351148148\n",
      "train loss:0.03450312724305383\n",
      "train loss:0.03913951240881524\n",
      "train loss:0.0047897818539790514\n",
      "train loss:0.02709055657366664\n",
      "train loss:0.019460485284867116\n",
      "train loss:0.020598033619545535\n",
      "train loss:0.01618589727750595\n",
      "train loss:0.03329185753522629\n",
      "train loss:0.012533376895460403\n",
      "train loss:0.011932688459872588\n",
      "train loss:0.03430858152767257\n",
      "train loss:0.023312829745544373\n",
      "train loss:0.009444270848696635\n",
      "train loss:0.039508327697312655\n",
      "train loss:0.009451546358947427\n",
      "train loss:0.03364732836454126\n",
      "train loss:0.010925539289463347\n",
      "train loss:0.018877261522314063\n",
      "train loss:0.03132777234910232\n",
      "train loss:0.019722340166489785\n",
      "train loss:0.012066927692198873\n",
      "train loss:0.01061929796144986\n",
      "train loss:0.05667494075759463\n",
      "train loss:0.019066420941266034\n",
      "train loss:0.014913230211824327\n",
      "train loss:0.011190154918669194\n",
      "train loss:0.058014040697597934\n",
      "train loss:0.008911823998752264\n",
      "train loss:0.022116772591937575\n",
      "train loss:0.05439678007479943\n",
      "train loss:0.007148731990789635\n",
      "=== epoch:19, train acc:0.99, test acc:0.955 ===\n",
      "train loss:0.01645148289687974\n",
      "train loss:0.02242452485968167\n",
      "train loss:0.037726719264679714\n",
      "train loss:0.024100369029845848\n",
      "train loss:0.009041062459638139\n",
      "train loss:0.015198247361085195\n",
      "train loss:0.018775381988269255\n",
      "train loss:0.03467551962809496\n",
      "train loss:0.020137983299575977\n",
      "train loss:0.009097796063806755\n",
      "train loss:0.02505914538112902\n",
      "train loss:0.011229399459504021\n",
      "train loss:0.03169404402602001\n",
      "train loss:0.015707813647365742\n",
      "train loss:0.009180436021994228\n",
      "train loss:0.028475926489890137\n",
      "train loss:0.012711737885836743\n",
      "train loss:0.010930992832369651\n",
      "train loss:0.01064977134074361\n",
      "train loss:0.015798331891401583\n",
      "train loss:0.011844236186046582\n",
      "train loss:0.012542239197738397\n",
      "train loss:0.023021953506516937\n",
      "train loss:0.010576145907722294\n",
      "train loss:0.009182534025136722\n",
      "train loss:0.013083963472134936\n",
      "train loss:0.03557312221732229\n",
      "train loss:0.01610087681645929\n",
      "train loss:0.01951146351263958\n",
      "train loss:0.01891749769279563\n",
      "train loss:0.01598419501834469\n",
      "train loss:0.013195203000824671\n",
      "train loss:0.008920422790938515\n",
      "train loss:0.011941794375067014\n",
      "train loss:0.016543133382028008\n",
      "train loss:0.010606272966844167\n",
      "train loss:0.0224769762100508\n",
      "train loss:0.024283392460738705\n",
      "train loss:0.004094615555516253\n",
      "train loss:0.006922949806204146\n",
      "train loss:0.010232876443999707\n",
      "train loss:0.009599363470044165\n",
      "train loss:0.05801436650275524\n",
      "train loss:0.026590459289829563\n",
      "train loss:0.011276058701490559\n",
      "train loss:0.03404187672461666\n",
      "train loss:0.03126340591846567\n",
      "train loss:0.024350799299527565\n",
      "train loss:0.024974604989574344\n",
      "train loss:0.05508682937637292\n",
      "=== epoch:20, train acc:0.995, test acc:0.954 ===\n",
      "train loss:0.016287200820162738\n",
      "train loss:0.008949099082326047\n",
      "train loss:0.026649673125841555\n",
      "train loss:0.004619426937849443\n",
      "train loss:0.007847796792309623\n",
      "train loss:0.01506230870883015\n",
      "train loss:0.01162877033653245\n",
      "train loss:0.011597980222855502\n",
      "train loss:0.010201538374237424\n",
      "train loss:0.015952518418048532\n",
      "train loss:0.009474097765360363\n",
      "train loss:0.006738008602390854\n",
      "train loss:0.03078366333460173\n",
      "train loss:0.011132091590758222\n",
      "train loss:0.021400733647112712\n",
      "train loss:0.0101998964743662\n",
      "train loss:0.0158644871260769\n",
      "train loss:0.014471018144203878\n",
      "train loss:0.010856794700245842\n",
      "train loss:0.031725539034197746\n",
      "train loss:0.020789753441089256\n",
      "train loss:0.0037774407576483377\n",
      "train loss:0.011553924745659208\n",
      "train loss:0.007503462743181712\n",
      "train loss:0.003399636946998507\n",
      "train loss:0.011608422897652158\n",
      "train loss:0.007909726573331223\n",
      "train loss:0.011998112665060621\n",
      "train loss:0.037794716748460654\n",
      "train loss:0.010102262791192773\n",
      "train loss:0.005582297286490196\n",
      "train loss:0.02770747929686375\n",
      "train loss:0.020177657460345416\n",
      "train loss:0.02509116467383802\n",
      "train loss:0.01745433270808312\n",
      "train loss:0.004408487361961\n",
      "train loss:0.00924343628761317\n",
      "train loss:0.007318033415024694\n",
      "train loss:0.011216792252143615\n",
      "train loss:0.015082621397916704\n",
      "train loss:0.014350834308202848\n",
      "train loss:0.005540150468325717\n",
      "train loss:0.030372463306351957\n",
      "train loss:0.014751985555079011\n",
      "train loss:0.010028309181805459\n",
      "train loss:0.012515321326229296\n",
      "train loss:0.00791748654350785\n",
      "train loss:0.010894220211834737\n",
      "train loss:0.007985120518803315\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.961\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from layers import *\n",
    "from gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "            filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db8dd5-5874-49f8-8287-90ebc1afd291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
