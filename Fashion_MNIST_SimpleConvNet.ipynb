{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9ff294-7d53-4404-bf73-ba0e5a1e4a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299954881515226\n",
      "=== epoch:1, train acc:0.119, test acc:0.099 ===\n",
      "train loss:2.29804267740831\n",
      "train loss:2.2936837021373897\n",
      "train loss:2.2912573629656126\n",
      "train loss:2.281889127572752\n",
      "train loss:2.268992077001456\n",
      "train loss:2.2546298810797194\n",
      "train loss:2.234259565764542\n",
      "train loss:2.218795197279899\n",
      "train loss:2.1837250536694914\n",
      "train loss:2.1624600300288765\n",
      "train loss:2.1312908365544305\n",
      "train loss:2.086288003543357\n",
      "train loss:2.0893970927250787\n",
      "train loss:2.0061073649609473\n",
      "train loss:1.943133911033385\n",
      "train loss:1.8676208349067052\n",
      "train loss:1.8431811752644414\n",
      "train loss:1.7496048922782412\n",
      "train loss:1.661025981668129\n",
      "train loss:1.6096392132682544\n",
      "train loss:1.4673839581285986\n",
      "train loss:1.4259400363912542\n",
      "train loss:1.3535521858341466\n",
      "train loss:1.3360580934829216\n",
      "train loss:1.2202656354878194\n",
      "train loss:1.1248169348767632\n",
      "train loss:1.0836317235243795\n",
      "train loss:1.0671509500276086\n",
      "train loss:0.8570401701952045\n",
      "train loss:0.775195337974346\n",
      "train loss:0.7992579975522163\n",
      "train loss:0.8540879673570587\n",
      "train loss:0.883659736442795\n",
      "train loss:0.7717530836687524\n",
      "train loss:0.585550564533892\n",
      "train loss:0.6285870666057585\n",
      "train loss:0.7270924522058408\n",
      "train loss:0.6976938115064515\n",
      "train loss:0.687572228150747\n",
      "train loss:0.6204587478754378\n",
      "train loss:0.7965950538665071\n",
      "train loss:0.6016403567613413\n",
      "train loss:0.6143900377597244\n",
      "train loss:0.6835047872752789\n",
      "train loss:0.5584108566291226\n",
      "train loss:0.4756064222777573\n",
      "train loss:0.517865558706989\n",
      "train loss:0.6632751525073524\n",
      "train loss:0.5054868378556469\n",
      "train loss:0.5815281506814709\n",
      "=== epoch:2, train acc:0.826, test acc:0.808 ===\n",
      "train loss:0.535790742454378\n",
      "train loss:0.4871388018556194\n",
      "train loss:0.5558999695688448\n",
      "train loss:0.5090108222139711\n",
      "train loss:0.37401402150304386\n",
      "train loss:0.4548933452141636\n",
      "train loss:0.4527334603854091\n",
      "train loss:0.4666317986094492\n",
      "train loss:0.3553127255964623\n",
      "train loss:0.3719555896878204\n",
      "train loss:0.4488592704439338\n",
      "train loss:0.5693553604619308\n",
      "train loss:0.3233911461432815\n",
      "train loss:0.4245768268631118\n",
      "train loss:0.3948496840165855\n",
      "train loss:0.36649296874769066\n",
      "train loss:0.3860653889621625\n",
      "train loss:0.4148517115759185\n",
      "train loss:0.33777503270807047\n",
      "train loss:0.5057198213109421\n",
      "train loss:0.44847591263691056\n",
      "train loss:0.40699277712519644\n",
      "train loss:0.31886808255818183\n",
      "train loss:0.318464407894175\n",
      "train loss:0.34763751492639594\n",
      "train loss:0.46945855251044877\n",
      "train loss:0.43642247973636805\n",
      "train loss:0.33611713089766826\n",
      "train loss:0.34621386108412366\n",
      "train loss:0.5443538312415493\n",
      "train loss:0.31234552379130787\n",
      "train loss:0.5260112425864816\n",
      "train loss:0.3669216746641803\n",
      "train loss:0.24342248989184578\n",
      "train loss:0.3683282283010545\n",
      "train loss:0.3550534915216179\n",
      "train loss:0.522848926210147\n",
      "train loss:0.24003731900703304\n",
      "train loss:0.4187989162981349\n",
      "train loss:0.36422694347045836\n",
      "train loss:0.3193286643662421\n",
      "train loss:0.47614239844838374\n",
      "train loss:0.38114471981306\n",
      "train loss:0.3394379020913098\n",
      "train loss:0.32181241692843243\n",
      "train loss:0.28489115506977775\n",
      "train loss:0.21453926956594052\n",
      "train loss:0.31937645560390765\n",
      "train loss:0.3873571485865639\n",
      "train loss:0.33624050560814994\n",
      "=== epoch:3, train acc:0.87, test acc:0.86 ===\n",
      "train loss:0.45134701836399777\n",
      "train loss:0.24822156349221747\n",
      "train loss:0.4726629312061625\n",
      "train loss:0.2846699170875422\n",
      "train loss:0.4044673833972274\n",
      "train loss:0.27978222647663376\n",
      "train loss:0.24898932666611093\n",
      "train loss:0.38533684828543635\n",
      "train loss:0.30471557535140376\n",
      "train loss:0.2615877066540871\n",
      "train loss:0.35414254759531\n",
      "train loss:0.34427351637551723\n",
      "train loss:0.21735598102324832\n",
      "train loss:0.2705594910285205\n",
      "train loss:0.32459911365575017\n",
      "train loss:0.37007398061782587\n",
      "train loss:0.2684336231326985\n",
      "train loss:0.2782199131340082\n",
      "train loss:0.3752697057822711\n",
      "train loss:0.2755932264620266\n",
      "train loss:0.462578628929512\n",
      "train loss:0.2839677024550198\n",
      "train loss:0.4442077029690546\n",
      "train loss:0.2968492212994543\n",
      "train loss:0.2995412149322165\n",
      "train loss:0.2744324552922686\n",
      "train loss:0.26875696669707577\n",
      "train loss:0.3013678116253016\n",
      "train loss:0.3064401379872941\n",
      "train loss:0.2777139518759139\n",
      "train loss:0.2569961864359328\n",
      "train loss:0.3198379803800867\n",
      "train loss:0.19205798540300542\n",
      "train loss:0.40349602756174135\n",
      "train loss:0.23157624682242983\n",
      "train loss:0.31463852316929786\n",
      "train loss:0.30890528616678803\n",
      "train loss:0.2683238378098421\n",
      "train loss:0.32964173389646234\n",
      "train loss:0.2681241314496117\n",
      "train loss:0.1766472212747202\n",
      "train loss:0.2343494803945439\n",
      "train loss:0.3816133699048705\n",
      "train loss:0.23368907380673373\n",
      "train loss:0.23271720661219836\n",
      "train loss:0.3155992105191558\n",
      "train loss:0.322104841784776\n",
      "train loss:0.3415887226739836\n",
      "train loss:0.15150085563306426\n",
      "train loss:0.28283627876489925\n",
      "=== epoch:4, train acc:0.897, test acc:0.881 ===\n",
      "train loss:0.24227100937137108\n",
      "train loss:0.3545783166575538\n",
      "train loss:0.2926904134516812\n",
      "train loss:0.27164018759096853\n",
      "train loss:0.1809309280633042\n",
      "train loss:0.22053922681885318\n",
      "train loss:0.17883288765320823\n",
      "train loss:0.24378641371766954\n",
      "train loss:0.2413914368753198\n",
      "train loss:0.10319620030377279\n",
      "train loss:0.259732428812439\n",
      "train loss:0.1594866547861877\n",
      "train loss:0.35254419762155753\n",
      "train loss:0.35030694616396063\n",
      "train loss:0.25145582716752385\n",
      "train loss:0.18376621963845818\n",
      "train loss:0.15543253850317595\n",
      "train loss:0.21778230003134616\n",
      "train loss:0.236927844665617\n",
      "train loss:0.2287696596546466\n",
      "train loss:0.3099463003594818\n",
      "train loss:0.36674834614798135\n",
      "train loss:0.2715058530612988\n",
      "train loss:0.2841404665216739\n",
      "train loss:0.34067273118193214\n",
      "train loss:0.18334970357840377\n",
      "train loss:0.19751665304819702\n",
      "train loss:0.2918664351588423\n",
      "train loss:0.22109830472298644\n",
      "train loss:0.20039026619698483\n",
      "train loss:0.18818761213634694\n",
      "train loss:0.11642787865392562\n",
      "train loss:0.15871493186002833\n",
      "train loss:0.2146583321882579\n",
      "train loss:0.1285124994174243\n",
      "train loss:0.2868511387132575\n",
      "train loss:0.38069835672621205\n",
      "train loss:0.28740684512047837\n",
      "train loss:0.20726383213499674\n",
      "train loss:0.29594244333315217\n",
      "train loss:0.35651429829425035\n",
      "train loss:0.2070037149404616\n",
      "train loss:0.2901470286138196\n",
      "train loss:0.2296933162977608\n",
      "train loss:0.21705886727953286\n",
      "train loss:0.28701664039805697\n",
      "train loss:0.22051080442870621\n",
      "train loss:0.2928035207901302\n",
      "train loss:0.2820792061710129\n",
      "train loss:0.24741006597311097\n",
      "=== epoch:5, train acc:0.924, test acc:0.895 ===\n",
      "train loss:0.26511240855559526\n",
      "train loss:0.3450528126614285\n",
      "train loss:0.14974630856577134\n",
      "train loss:0.14157978582443606\n",
      "train loss:0.1697525279142075\n",
      "train loss:0.14954909100466998\n",
      "train loss:0.20481692894130885\n",
      "train loss:0.1813172581054275\n",
      "train loss:0.20763794617613965\n",
      "train loss:0.20006987686696115\n",
      "train loss:0.24035397195572245\n",
      "train loss:0.26927161315753684\n",
      "train loss:0.20650963026992486\n",
      "train loss:0.2639335118993321\n",
      "train loss:0.23827167015479056\n",
      "train loss:0.26186399113825093\n",
      "train loss:0.292096768586648\n",
      "train loss:0.25281284938507587\n",
      "train loss:0.11954964010282515\n",
      "train loss:0.2259451575987007\n",
      "train loss:0.20596134076218095\n",
      "train loss:0.15755404063971626\n",
      "train loss:0.21188839782493002\n",
      "train loss:0.3257827451992451\n",
      "train loss:0.16363226149654217\n",
      "train loss:0.2004453665588491\n",
      "train loss:0.18409326918270236\n",
      "train loss:0.20477607535142073\n",
      "train loss:0.2203510417283838\n",
      "train loss:0.257607271065401\n",
      "train loss:0.258684727359235\n",
      "train loss:0.22052988934435255\n",
      "train loss:0.20629236405264986\n",
      "train loss:0.21215274527597558\n",
      "train loss:0.21679098415447082\n",
      "train loss:0.1963627232615702\n",
      "train loss:0.22932658948790752\n",
      "train loss:0.09683342503933591\n",
      "train loss:0.20175302936521888\n",
      "train loss:0.2645145825043766\n",
      "train loss:0.1189732111136746\n",
      "train loss:0.1874888944152169\n",
      "train loss:0.23738079539080156\n",
      "train loss:0.1866618872137551\n",
      "train loss:0.1749768923194699\n",
      "train loss:0.08188361773145099\n",
      "train loss:0.26143268010158915\n",
      "train loss:0.1697891885572636\n",
      "train loss:0.17207384831198846\n",
      "train loss:0.2492736875027159\n",
      "=== epoch:6, train acc:0.93, test acc:0.902 ===\n",
      "train loss:0.09484153315163725\n",
      "train loss:0.38503118588494845\n",
      "train loss:0.18268615149607254\n",
      "train loss:0.19156535092950974\n",
      "train loss:0.12555321198352676\n",
      "train loss:0.16303898475254414\n",
      "train loss:0.2053973424557054\n",
      "train loss:0.28212415514613665\n",
      "train loss:0.22261807356587643\n",
      "train loss:0.15337844227987202\n",
      "train loss:0.23770724396496756\n",
      "train loss:0.25869949767233835\n",
      "train loss:0.24853927902831846\n",
      "train loss:0.13701725688054295\n",
      "train loss:0.13317868797951374\n",
      "train loss:0.18896169919235695\n",
      "train loss:0.14345076174461713\n",
      "train loss:0.13048820167362604\n",
      "train loss:0.14644266648719495\n",
      "train loss:0.2131038792628246\n",
      "train loss:0.14254291480504133\n",
      "train loss:0.43592459328435873\n",
      "train loss:0.18263677893661565\n",
      "train loss:0.07468740752308443\n",
      "train loss:0.23561112518110644\n",
      "train loss:0.1748114067373764\n",
      "train loss:0.2508188812751768\n",
      "train loss:0.1865492989768442\n",
      "train loss:0.19960368740744136\n",
      "train loss:0.16620898438769285\n",
      "train loss:0.21938247446024628\n",
      "train loss:0.146674770691327\n",
      "train loss:0.2669637445998127\n",
      "train loss:0.29907707652319065\n",
      "train loss:0.15659737404456922\n",
      "train loss:0.14519021237266103\n",
      "train loss:0.17486470923790776\n",
      "train loss:0.13910272052920847\n",
      "train loss:0.20499000483290422\n",
      "train loss:0.215406734325781\n",
      "train loss:0.173378813303291\n",
      "train loss:0.13261461060976415\n",
      "train loss:0.12040499528211958\n",
      "train loss:0.12282490327370149\n",
      "train loss:0.12559604558082074\n",
      "train loss:0.17741585056959644\n",
      "train loss:0.1575004640311237\n",
      "train loss:0.1323304611279285\n",
      "train loss:0.13508101657261984\n",
      "train loss:0.07591951156703995\n",
      "=== epoch:7, train acc:0.926, test acc:0.916 ===\n",
      "train loss:0.21261716001572886\n",
      "train loss:0.18734020466706866\n",
      "train loss:0.15392536122948447\n",
      "train loss:0.33452975479457203\n",
      "train loss:0.2048678948183294\n",
      "train loss:0.18681677248578102\n",
      "train loss:0.11235636889434604\n",
      "train loss:0.12190929216696843\n",
      "train loss:0.10751384698177983\n",
      "train loss:0.10817620973725418\n",
      "train loss:0.23703983621722766\n",
      "train loss:0.17794897246966815\n",
      "train loss:0.17181393130354525\n",
      "train loss:0.1841846026537118\n",
      "train loss:0.13875062346362182\n",
      "train loss:0.1749633987402016\n",
      "train loss:0.15687135915116987\n",
      "train loss:0.08332036280411378\n",
      "train loss:0.09382370736470211\n",
      "train loss:0.2283646499938044\n",
      "train loss:0.15095324108437236\n",
      "train loss:0.15067060786066935\n",
      "train loss:0.16587810383860466\n",
      "train loss:0.11007921867809972\n",
      "train loss:0.3090671209155924\n",
      "train loss:0.11044486430647765\n",
      "train loss:0.1152512291818934\n",
      "train loss:0.11748015603638524\n",
      "train loss:0.15600994689916067\n",
      "train loss:0.11698997813096992\n",
      "train loss:0.18151032683460527\n",
      "train loss:0.13547406980815568\n",
      "train loss:0.09348608604773977\n",
      "train loss:0.10105699094005088\n",
      "train loss:0.14342360864525117\n",
      "train loss:0.20299674147389613\n",
      "train loss:0.15312271032238833\n",
      "train loss:0.15990151970442668\n",
      "train loss:0.09935769564522325\n",
      "train loss:0.09990211772616718\n",
      "train loss:0.22781863977347783\n",
      "train loss:0.1823116859184461\n",
      "train loss:0.1561924659826417\n",
      "train loss:0.1849984293820383\n",
      "train loss:0.220497321272605\n",
      "train loss:0.12007593941536125\n",
      "train loss:0.21536940266447396\n",
      "train loss:0.23003485597204684\n",
      "train loss:0.16048323961875022\n",
      "train loss:0.08207330820291021\n",
      "=== epoch:8, train acc:0.95, test acc:0.928 ===\n",
      "train loss:0.057688187532812164\n",
      "train loss:0.12552025453053078\n",
      "train loss:0.16061112587567472\n",
      "train loss:0.13996071380756647\n",
      "train loss:0.2520737297601547\n",
      "train loss:0.15073765672874942\n",
      "train loss:0.08265813706202442\n",
      "train loss:0.14361719806122195\n",
      "train loss:0.06360942948332456\n",
      "train loss:0.11035197581042175\n",
      "train loss:0.1177633185121921\n",
      "train loss:0.17456392636795084\n",
      "train loss:0.276642040617019\n",
      "train loss:0.10627038865662879\n",
      "train loss:0.14137667281218305\n",
      "train loss:0.1352872736822175\n",
      "train loss:0.10186113063840077\n",
      "train loss:0.11918859192822233\n",
      "train loss:0.25399443322511395\n",
      "train loss:0.16137699289847357\n",
      "train loss:0.1672206751221006\n",
      "train loss:0.14910900599851662\n",
      "train loss:0.06235172008511128\n",
      "train loss:0.1133875113950507\n",
      "train loss:0.0404809563747256\n",
      "train loss:0.09142019737002706\n",
      "train loss:0.20062418141876062\n",
      "train loss:0.08563648475529087\n",
      "train loss:0.145924232979241\n",
      "train loss:0.1392707093374192\n",
      "train loss:0.057717548712305604\n",
      "train loss:0.09144576304831331\n",
      "train loss:0.08738413819424426\n",
      "train loss:0.2247058504001078\n",
      "train loss:0.1411939617061164\n",
      "train loss:0.07793114815431434\n",
      "train loss:0.0800010647266516\n",
      "train loss:0.16193782377430663\n",
      "train loss:0.1738174670427417\n",
      "train loss:0.08415418541297048\n",
      "train loss:0.1335400836251901\n",
      "train loss:0.1485525805884583\n",
      "train loss:0.0790477528618163\n",
      "train loss:0.15607714415917628\n",
      "train loss:0.11900564061074274\n",
      "train loss:0.08563957327040198\n",
      "train loss:0.060433667805566145\n",
      "train loss:0.13319096607200154\n",
      "train loss:0.1318700276390611\n",
      "train loss:0.12741376478238103\n",
      "=== epoch:9, train acc:0.958, test acc:0.936 ===\n",
      "train loss:0.10873346710652615\n",
      "train loss:0.09247071099706755\n",
      "train loss:0.17286471545162216\n",
      "train loss:0.13967886713710065\n",
      "train loss:0.17049873589594838\n",
      "train loss:0.18184831123682804\n",
      "train loss:0.15421330157877336\n",
      "train loss:0.0944189479781466\n",
      "train loss:0.09000652175260182\n",
      "train loss:0.1115342811196043\n",
      "train loss:0.2063399936374364\n",
      "train loss:0.11579240626537368\n",
      "train loss:0.08152205848050777\n",
      "train loss:0.11101983329689596\n",
      "train loss:0.06504174920965662\n",
      "train loss:0.0878404813165841\n",
      "train loss:0.11003644124957714\n",
      "train loss:0.10406089132916005\n",
      "train loss:0.047208661893249425\n",
      "train loss:0.11665721455305723\n",
      "train loss:0.06848620545436974\n",
      "train loss:0.17380510594761334\n",
      "train loss:0.07605592521398745\n",
      "train loss:0.11691389252278483\n",
      "train loss:0.0944135230026637\n",
      "train loss:0.14331702643437125\n",
      "train loss:0.204553290879439\n",
      "train loss:0.14415771711321052\n",
      "train loss:0.11207494207559505\n",
      "train loss:0.12731162882623462\n",
      "train loss:0.20990552924983358\n",
      "train loss:0.09865707581985884\n",
      "train loss:0.10155435985196748\n",
      "train loss:0.08420361583343454\n",
      "train loss:0.07734802522883147\n",
      "train loss:0.1302524711679547\n",
      "train loss:0.18217785417908533\n",
      "train loss:0.06383776727434545\n",
      "train loss:0.15519254773231164\n",
      "train loss:0.11428189983120193\n",
      "train loss:0.1024392734198013\n",
      "train loss:0.0834207492357961\n",
      "train loss:0.09791998600607138\n",
      "train loss:0.07744162836891152\n",
      "train loss:0.06104155126231848\n",
      "train loss:0.04965027438429199\n",
      "train loss:0.11094811903608669\n",
      "train loss:0.1082219400332987\n",
      "train loss:0.13525144465256886\n",
      "train loss:0.06983608755340584\n",
      "=== epoch:10, train acc:0.963, test acc:0.951 ===\n",
      "train loss:0.12200972270537784\n",
      "train loss:0.05675017760503294\n",
      "train loss:0.07012304826458701\n",
      "train loss:0.1130412287509922\n",
      "train loss:0.3333226109242503\n",
      "train loss:0.14518538073795664\n",
      "train loss:0.09699362410436413\n",
      "train loss:0.07580979148905413\n",
      "train loss:0.0880993356811501\n",
      "train loss:0.1546693290150918\n",
      "train loss:0.21326645941409839\n",
      "train loss:0.05983491952483872\n",
      "train loss:0.09234656566805213\n",
      "train loss:0.053598377094324974\n",
      "train loss:0.07065735175896233\n",
      "train loss:0.08062411890724955\n",
      "train loss:0.13491274378884108\n",
      "train loss:0.07838169817211249\n",
      "train loss:0.08505143462121996\n",
      "train loss:0.10626408709642503\n",
      "train loss:0.08892372791230953\n",
      "train loss:0.0851211137589707\n",
      "train loss:0.11676880841768647\n",
      "train loss:0.16505945878785203\n",
      "train loss:0.1029774252482088\n",
      "train loss:0.10632117475368741\n",
      "train loss:0.045589143914692626\n",
      "train loss:0.14272554004717808\n",
      "train loss:0.1135523935913651\n",
      "train loss:0.17379131106912335\n",
      "train loss:0.17214358390089207\n",
      "train loss:0.13544907761499125\n",
      "train loss:0.0769422256223159\n",
      "train loss:0.060151478130901465\n",
      "train loss:0.0919594633330924\n",
      "train loss:0.11833881488027012\n",
      "train loss:0.06979980305085516\n",
      "train loss:0.13422325549903302\n",
      "train loss:0.04467584332334438\n",
      "train loss:0.0931491441733602\n",
      "train loss:0.08778461970729318\n",
      "train loss:0.11689987017611449\n",
      "train loss:0.07265433975374778\n",
      "train loss:0.20613750175870646\n",
      "train loss:0.08593384058168795\n",
      "train loss:0.1379516802174456\n",
      "train loss:0.06851553289120847\n",
      "train loss:0.04663247015904513\n",
      "train loss:0.0813589202864507\n",
      "train loss:0.05939787595913473\n",
      "=== epoch:11, train acc:0.961, test acc:0.941 ===\n",
      "train loss:0.15150139507622562\n",
      "train loss:0.06876697932921026\n",
      "train loss:0.09191928873641482\n",
      "train loss:0.05501122055202393\n",
      "train loss:0.058912623642227616\n",
      "train loss:0.07808684515138836\n",
      "train loss:0.04780909761804975\n",
      "train loss:0.04778675502432822\n",
      "train loss:0.07169918657180956\n",
      "train loss:0.0993470359831253\n",
      "train loss:0.03368564898053343\n",
      "train loss:0.07578251146326848\n",
      "train loss:0.06711949162313795\n",
      "train loss:0.13666644976304848\n",
      "train loss:0.10385345317038785\n",
      "train loss:0.05123721633558825\n",
      "train loss:0.04313165383484506\n",
      "train loss:0.0409785816156887\n",
      "train loss:0.0326908992144491\n",
      "train loss:0.06103557216647732\n",
      "train loss:0.09355979987376668\n",
      "train loss:0.04352696798535128\n",
      "train loss:0.04694774376784146\n",
      "train loss:0.14429792155484988\n",
      "train loss:0.07244651064856808\n",
      "train loss:0.05597020638439512\n",
      "train loss:0.034672564255666734\n",
      "train loss:0.10906879545810541\n",
      "train loss:0.08176580159149427\n",
      "train loss:0.12911874341348875\n",
      "train loss:0.06437864110848997\n",
      "train loss:0.13051996380930853\n",
      "train loss:0.04393630755419203\n",
      "train loss:0.0980290454303479\n",
      "train loss:0.17096400819507668\n",
      "train loss:0.11004800023780419\n",
      "train loss:0.0774857322484977\n",
      "train loss:0.0359102849205726\n",
      "train loss:0.1267968012708106\n",
      "train loss:0.0436553036925687\n",
      "train loss:0.07779997796386039\n",
      "train loss:0.19592884847524983\n",
      "train loss:0.12194134675326306\n",
      "train loss:0.1610430106816554\n",
      "train loss:0.039998509407143154\n",
      "train loss:0.06050318337911352\n",
      "train loss:0.10690008345979823\n",
      "train loss:0.09045599516105328\n",
      "train loss:0.03547000969761542\n",
      "train loss:0.05460824872228507\n",
      "=== epoch:12, train acc:0.971, test acc:0.951 ===\n",
      "train loss:0.08758714598406524\n",
      "train loss:0.11162302503615859\n",
      "train loss:0.07076411926628466\n",
      "train loss:0.022864720985302747\n",
      "train loss:0.16446933595662844\n",
      "train loss:0.09531364892798094\n",
      "train loss:0.08254838821543309\n",
      "train loss:0.08034037399669179\n",
      "train loss:0.05695407777975252\n",
      "train loss:0.0925963851967854\n",
      "train loss:0.06109797821291129\n",
      "train loss:0.043581452065516\n",
      "train loss:0.08404876344356657\n",
      "train loss:0.06674379999243164\n",
      "train loss:0.05790900632945767\n",
      "train loss:0.02560597545044163\n",
      "train loss:0.06978648935450729\n",
      "train loss:0.09402326038594308\n",
      "train loss:0.05424595147856784\n",
      "train loss:0.06844636062136505\n",
      "train loss:0.02866685604784883\n",
      "train loss:0.015966900431867808\n",
      "train loss:0.05725563580622253\n",
      "train loss:0.061175366825791035\n",
      "train loss:0.10219828359559167\n",
      "train loss:0.04728962510351143\n",
      "train loss:0.14848470717006218\n",
      "train loss:0.036530644475358545\n",
      "train loss:0.13440838838661945\n",
      "train loss:0.05929991947223436\n",
      "train loss:0.07514867801040852\n",
      "train loss:0.023983160846107375\n",
      "train loss:0.0432003225894291\n",
      "train loss:0.05166781907714088\n",
      "train loss:0.12701546604658343\n",
      "train loss:0.05301770147192367\n",
      "train loss:0.06590860917874332\n",
      "train loss:0.029115828791518347\n",
      "train loss:0.14105659193438946\n",
      "train loss:0.06916122114598508\n",
      "train loss:0.06615397697250343\n",
      "train loss:0.046114395662098594\n",
      "train loss:0.07723856955253328\n",
      "train loss:0.1090442319473237\n",
      "train loss:0.09986892955843438\n",
      "train loss:0.07486288424479193\n",
      "train loss:0.09843058684801047\n",
      "train loss:0.1227597355894237\n",
      "train loss:0.07419890805135222\n",
      "train loss:0.04190404941767453\n",
      "=== epoch:13, train acc:0.971, test acc:0.957 ===\n",
      "train loss:0.07244074697092408\n",
      "train loss:0.031773887762543106\n",
      "train loss:0.12459627648945787\n",
      "train loss:0.03327921295125909\n",
      "train loss:0.09087456082822845\n",
      "train loss:0.17388673652100206\n",
      "train loss:0.04120385545709006\n",
      "train loss:0.13310016102726144\n",
      "train loss:0.056386011771414946\n",
      "train loss:0.1545965145257072\n",
      "train loss:0.06650375118537184\n",
      "train loss:0.042336113815907926\n",
      "train loss:0.04733749654780366\n",
      "train loss:0.1806693332731806\n",
      "train loss:0.06557913185134157\n",
      "train loss:0.04946723098071625\n",
      "train loss:0.05112988168114364\n",
      "train loss:0.05026997455783769\n",
      "train loss:0.07053616601727372\n",
      "train loss:0.0608751249157575\n",
      "train loss:0.022844116413273677\n",
      "train loss:0.05178432558033231\n",
      "train loss:0.0501908736689292\n",
      "train loss:0.058501986491707625\n",
      "train loss:0.04216746490823575\n",
      "train loss:0.043578963491335145\n",
      "train loss:0.0653479201411668\n",
      "train loss:0.07084914364975899\n",
      "train loss:0.03683968676000299\n",
      "train loss:0.07661914833572413\n",
      "train loss:0.05300317604165977\n",
      "train loss:0.028838484943694738\n",
      "train loss:0.1693183558804723\n",
      "train loss:0.05385602367716521\n",
      "train loss:0.033866442777816114\n",
      "train loss:0.0441799611342363\n",
      "train loss:0.07754577618600533\n",
      "train loss:0.07658204007258802\n",
      "train loss:0.01938980568107929\n",
      "train loss:0.1009185198483476\n",
      "train loss:0.07753623804945285\n",
      "train loss:0.04730893582945626\n",
      "train loss:0.03699424720862709\n",
      "train loss:0.09855296684916043\n",
      "train loss:0.08517677551446995\n",
      "train loss:0.022504114025374576\n",
      "train loss:0.056531903106787455\n",
      "train loss:0.01530057432904497\n",
      "train loss:0.028487314470358207\n",
      "train loss:0.10014137159899748\n",
      "=== epoch:14, train acc:0.975, test acc:0.946 ===\n",
      "train loss:0.03674318012025171\n",
      "train loss:0.06961590576792621\n",
      "train loss:0.03462350663652793\n",
      "train loss:0.02768027832018525\n",
      "train loss:0.026388580644839562\n",
      "train loss:0.029892705633090363\n",
      "train loss:0.03698524300716982\n",
      "train loss:0.031993720916083254\n",
      "train loss:0.07163527563640609\n",
      "train loss:0.025317361034503914\n",
      "train loss:0.05173021731376016\n",
      "train loss:0.08234601155516144\n",
      "train loss:0.06471494311099907\n",
      "train loss:0.03671936222850318\n",
      "train loss:0.023871025309623664\n",
      "train loss:0.10113167956334085\n",
      "train loss:0.04219079721641842\n",
      "train loss:0.023077675968260117\n",
      "train loss:0.027901197699425638\n",
      "train loss:0.06197128770042644\n",
      "train loss:0.03778796576861288\n",
      "train loss:0.044781887423599835\n",
      "train loss:0.021240049330380976\n",
      "train loss:0.10019161376554012\n",
      "train loss:0.023414224047338762\n",
      "train loss:0.08726128025320748\n",
      "train loss:0.11988212617398383\n",
      "train loss:0.04153718385157593\n",
      "train loss:0.05420829347432831\n",
      "train loss:0.11422937903080953\n",
      "train loss:0.04297663501259818\n",
      "train loss:0.0589189172478156\n",
      "train loss:0.053906552077066176\n",
      "train loss:0.12607901853789164\n",
      "train loss:0.06588148611120276\n",
      "train loss:0.10525120274521531\n",
      "train loss:0.04783952358261561\n",
      "train loss:0.06833535726457407\n",
      "train loss:0.06644588519001256\n",
      "train loss:0.058847584555574005\n",
      "train loss:0.12496451710645408\n",
      "train loss:0.056282500459229196\n",
      "train loss:0.021978198149491705\n",
      "train loss:0.048284467954813816\n",
      "train loss:0.07425878124254762\n",
      "train loss:0.05440330287472397\n",
      "train loss:0.06680398663917898\n",
      "train loss:0.031092237836160844\n",
      "train loss:0.02151916780468325\n",
      "train loss:0.03168493589133055\n",
      "=== epoch:15, train acc:0.981, test acc:0.96 ===\n",
      "train loss:0.02887841815239007\n",
      "train loss:0.019722903933780133\n",
      "train loss:0.03476059364513612\n",
      "train loss:0.05450782899312204\n",
      "train loss:0.08437795807476933\n",
      "train loss:0.020506683664780246\n",
      "train loss:0.05843343668312501\n",
      "train loss:0.04821854132944679\n",
      "train loss:0.19079291940366008\n",
      "train loss:0.020835550619316884\n",
      "train loss:0.03680256262613799\n",
      "train loss:0.05690475187355089\n",
      "train loss:0.024608827137727492\n",
      "train loss:0.03391917978934933\n",
      "train loss:0.07872229118699811\n",
      "train loss:0.01919900681429349\n",
      "train loss:0.05525926069294166\n",
      "train loss:0.05305154202139997\n",
      "train loss:0.03729181826348118\n",
      "train loss:0.020903784701022873\n",
      "train loss:0.09439270258237899\n",
      "train loss:0.056423483882963615\n",
      "train loss:0.09728551315420589\n",
      "train loss:0.0655138726978645\n",
      "train loss:0.024996436336049848\n",
      "train loss:0.03424736140607653\n",
      "train loss:0.06094614846791683\n",
      "train loss:0.04276924090088185\n",
      "train loss:0.08166580517939678\n",
      "train loss:0.018873875520905626\n",
      "train loss:0.03128959728991179\n",
      "train loss:0.055748100203814994\n",
      "train loss:0.04345381110307423\n",
      "train loss:0.04773574482567632\n",
      "train loss:0.041682443740380305\n",
      "train loss:0.07807855104464961\n",
      "train loss:0.06840717858918967\n",
      "train loss:0.022712816634618455\n",
      "train loss:0.04475168955767568\n",
      "train loss:0.017982014967220086\n",
      "train loss:0.019772404716590107\n",
      "train loss:0.016958041445232375\n",
      "train loss:0.0214478600951667\n",
      "train loss:0.021458763529850194\n",
      "train loss:0.03054736089016271\n",
      "train loss:0.03834858500968283\n",
      "train loss:0.02834341686513177\n",
      "train loss:0.03991833821846058\n",
      "train loss:0.031242940843266467\n",
      "train loss:0.05706874160754292\n",
      "=== epoch:16, train acc:0.984, test acc:0.963 ===\n",
      "train loss:0.06284342542158387\n",
      "train loss:0.032002097734982825\n",
      "train loss:0.011280295089620616\n",
      "train loss:0.040251755328100505\n",
      "train loss:0.017121477968896175\n",
      "train loss:0.06676452313789603\n",
      "train loss:0.07132877280416165\n",
      "train loss:0.08812041520475501\n",
      "train loss:0.032874663290424606\n",
      "train loss:0.012915915500214125\n",
      "train loss:0.027466899847991624\n",
      "train loss:0.023796608981539912\n",
      "train loss:0.12605470955045128\n",
      "train loss:0.06242679356589281\n",
      "train loss:0.02396119764234493\n",
      "train loss:0.03745133613801257\n",
      "train loss:0.015263070259758727\n",
      "train loss:0.017848851277171472\n",
      "train loss:0.022684677233398537\n",
      "train loss:0.03609068333033182\n",
      "train loss:0.030432109317777227\n",
      "train loss:0.01309354344570016\n",
      "train loss:0.0504223137573036\n",
      "train loss:0.04478161389047289\n",
      "train loss:0.034301842997644605\n",
      "train loss:0.03993474907678669\n",
      "train loss:0.019131063291814857\n",
      "train loss:0.024752308064653384\n",
      "train loss:0.023951009593615683\n",
      "train loss:0.031945524351807794\n",
      "train loss:0.02133756795410723\n",
      "train loss:0.05864566627702173\n",
      "train loss:0.014014944825678867\n",
      "train loss:0.034847154128735756\n",
      "train loss:0.020406553242608497\n",
      "train loss:0.04613704155570164\n",
      "train loss:0.032381694575565195\n",
      "train loss:0.029010404125821412\n",
      "train loss:0.028607859853327113\n",
      "train loss:0.04831465978388563\n",
      "train loss:0.02301372596151209\n",
      "train loss:0.016991051315165224\n",
      "train loss:0.04208477629550246\n",
      "train loss:0.024103917610197288\n",
      "train loss:0.059018547831552054\n",
      "train loss:0.06527914949584798\n",
      "train loss:0.046910130307205325\n",
      "train loss:0.030763559667529753\n",
      "train loss:0.0340823268731003\n",
      "train loss:0.025462239680416273\n",
      "=== epoch:17, train acc:0.988, test acc:0.961 ===\n",
      "train loss:0.03428759885905201\n",
      "train loss:0.0637640825621923\n",
      "train loss:0.030897501189279888\n",
      "train loss:0.01250517850444258\n",
      "train loss:0.05593665457095907\n",
      "train loss:0.044972279660806365\n",
      "train loss:0.014213997769381538\n",
      "train loss:0.007789872707123197\n",
      "train loss:0.04363979030604363\n",
      "train loss:0.02323739185870695\n",
      "train loss:0.026626403626028198\n",
      "train loss:0.06373536982699597\n",
      "train loss:0.0139206033397203\n",
      "train loss:0.027366401258070102\n",
      "train loss:0.02993781110464274\n",
      "train loss:0.02500874930777243\n",
      "train loss:0.04075004841101407\n",
      "train loss:0.04215096485519194\n",
      "train loss:0.016612641586582332\n",
      "train loss:0.020216505433959406\n",
      "train loss:0.043458326864470785\n",
      "train loss:0.01314118090915069\n",
      "train loss:0.0231116638653784\n",
      "train loss:0.05430418980673586\n",
      "train loss:0.032439670785264156\n",
      "train loss:0.036150956063826144\n",
      "train loss:0.02652639645655145\n",
      "train loss:0.03425713350047917\n",
      "train loss:0.019758334861593663\n",
      "train loss:0.045345511736145185\n",
      "train loss:0.019667055721679805\n",
      "train loss:0.03197987000026853\n",
      "train loss:0.01893563652059978\n",
      "train loss:0.011121106011181965\n",
      "train loss:0.0673960812005432\n",
      "train loss:0.017191835620134325\n",
      "train loss:0.07796332476890722\n",
      "train loss:0.021139064964467758\n",
      "train loss:0.009587194846641367\n",
      "train loss:0.014892569173096006\n",
      "train loss:0.037866320215311194\n",
      "train loss:0.023426787254933853\n",
      "train loss:0.017604839043902958\n",
      "train loss:0.013341153485087328\n",
      "train loss:0.018663298032821796\n",
      "train loss:0.04397427206491684\n",
      "train loss:0.021257127708712898\n",
      "train loss:0.01039565114820923\n",
      "train loss:0.051283330862266316\n",
      "train loss:0.01179005369149698\n",
      "=== epoch:18, train acc:0.989, test acc:0.964 ===\n",
      "train loss:0.0628685984717807\n",
      "train loss:0.03430827271367981\n",
      "train loss:0.027736040682544684\n",
      "train loss:0.024110686895254165\n",
      "train loss:0.023928494980569228\n",
      "train loss:0.04611516970319352\n",
      "train loss:0.02955053547352583\n",
      "train loss:0.10549848580840923\n",
      "train loss:0.013523100969272318\n",
      "train loss:0.04134883855664807\n",
      "train loss:0.03447283013285383\n",
      "train loss:0.02411704232464539\n",
      "train loss:0.018653129174908432\n",
      "train loss:0.06179734137273319\n",
      "train loss:0.05309994557544842\n",
      "train loss:0.04292235363020811\n",
      "train loss:0.02080563287629633\n",
      "train loss:0.015573816797438205\n",
      "train loss:0.0059767627179064635\n",
      "train loss:0.03375301525033628\n",
      "train loss:0.039412242352016985\n",
      "train loss:0.015253475281720375\n",
      "train loss:0.05447780367205347\n",
      "train loss:0.007654914359800899\n",
      "train loss:0.053675928035872796\n",
      "train loss:0.02004492464681537\n",
      "train loss:0.019645631560355977\n",
      "train loss:0.013829761796034925\n",
      "train loss:0.013205933349306427\n",
      "train loss:0.06848019269244614\n",
      "train loss:0.017135849889871883\n",
      "train loss:0.010106538292636476\n",
      "train loss:0.02128024846496982\n",
      "train loss:0.03171706227456864\n",
      "train loss:0.031939105843963686\n",
      "train loss:0.022664142647580418\n",
      "train loss:0.02339995580605615\n",
      "train loss:0.03205440089607139\n",
      "train loss:0.02284419496899129\n",
      "train loss:0.04135974744684374\n",
      "train loss:0.023544903490600896\n",
      "train loss:0.04534116853803276\n",
      "train loss:0.013547397800619955\n",
      "train loss:0.06901091663663887\n",
      "train loss:0.016249012684246465\n",
      "train loss:0.009712536995658112\n",
      "train loss:0.034638891113881465\n",
      "train loss:0.005937345320148213\n",
      "train loss:0.06877693457971815\n",
      "train loss:0.011239756025940191\n",
      "=== epoch:19, train acc:0.989, test acc:0.961 ===\n",
      "train loss:0.035723971556589865\n",
      "train loss:0.0136711411669611\n",
      "train loss:0.05598554961384382\n",
      "train loss:0.016373989881134282\n",
      "train loss:0.019088490125999075\n",
      "train loss:0.05203175683404893\n",
      "train loss:0.015825416814647938\n",
      "train loss:0.03161801292361213\n",
      "train loss:0.029042098305749543\n",
      "train loss:0.01759165566661679\n",
      "train loss:0.026559331517650178\n",
      "train loss:0.01215422784437758\n",
      "train loss:0.02936967506333881\n",
      "train loss:0.023648487706276228\n",
      "train loss:0.013535795536049238\n",
      "train loss:0.018856469462385516\n",
      "train loss:0.022086307240874424\n",
      "train loss:0.022488126992102613\n",
      "train loss:0.02381165257835771\n",
      "train loss:0.010470999196634167\n",
      "train loss:0.02404150636541775\n",
      "train loss:0.019537053347815588\n",
      "train loss:0.03438169840266936\n",
      "train loss:0.014893275758731069\n",
      "train loss:0.016656474909001476\n",
      "train loss:0.025069541156765216\n",
      "train loss:0.010467312075817843\n",
      "train loss:0.00974563703114285\n",
      "train loss:0.03222772820036128\n",
      "train loss:0.015201415568758625\n",
      "train loss:0.02361467813739161\n",
      "train loss:0.013971166756758018\n",
      "train loss:0.017271359073243005\n",
      "train loss:0.015446901858846125\n",
      "train loss:0.0230214778207869\n",
      "train loss:0.031077747198612643\n",
      "train loss:0.02394888238858514\n",
      "train loss:0.09724089951710564\n",
      "train loss:0.009561620752165843\n",
      "train loss:0.01789542345588346\n",
      "train loss:0.015665924234247737\n",
      "train loss:0.016294540315792423\n",
      "train loss:0.023798749971641112\n",
      "train loss:0.036939216393862356\n",
      "train loss:0.029370812099216978\n",
      "train loss:0.01310531283731272\n",
      "train loss:0.02004680616459501\n",
      "train loss:0.016864844669771516\n",
      "train loss:0.040734970656764624\n",
      "train loss:0.017920189736770915\n",
      "=== epoch:20, train acc:0.995, test acc:0.961 ===\n",
      "train loss:0.011366398676672098\n",
      "train loss:0.017098929314280382\n",
      "train loss:0.01673261380286557\n",
      "train loss:0.004968444634481446\n",
      "train loss:0.00920275163330614\n",
      "train loss:0.018993717875700047\n",
      "train loss:0.032974853666008816\n",
      "train loss:0.014554291862231214\n",
      "train loss:0.011972972358754809\n",
      "train loss:0.022510881367410383\n",
      "train loss:0.010603066200676524\n",
      "train loss:0.01846792697170084\n",
      "train loss:0.011571213239255352\n",
      "train loss:0.015223866419732899\n",
      "train loss:0.03245607527114516\n",
      "train loss:0.04898303039622955\n",
      "train loss:0.03999968865558958\n",
      "train loss:0.01035383359061936\n",
      "train loss:0.03136730621155715\n",
      "train loss:0.015788588150215478\n",
      "train loss:0.021486488133154947\n",
      "train loss:0.025869554949696113\n",
      "train loss:0.02660272580626491\n",
      "train loss:0.0168914267987836\n",
      "train loss:0.01173335055405227\n",
      "train loss:0.013024780879753641\n",
      "train loss:0.017486421388485017\n",
      "train loss:0.02670468662393477\n",
      "train loss:0.011329129914747786\n",
      "train loss:0.02025849386145498\n",
      "train loss:0.01925480098270387\n",
      "train loss:0.026559370516565234\n",
      "train loss:0.01260093121790952\n",
      "train loss:0.007991808483999564\n",
      "train loss:0.019456129685390785\n",
      "train loss:0.014609307382856585\n",
      "train loss:0.03760198530998167\n",
      "train loss:0.010590334031909667\n",
      "train loss:0.016946647099472134\n",
      "train loss:0.021443927821367524\n",
      "train loss:0.017630660735989626\n",
      "train loss:0.011083268474967855\n",
      "train loss:0.01277418423599699\n",
      "train loss:0.006249974182690707\n",
      "train loss:0.02360502869577961\n",
      "train loss:0.012426501514793529\n",
      "train loss:0.01598245669243205\n",
      "train loss:0.017768521447761115\n",
      "train loss:0.01066134461577287\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.969\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from layers import *\n",
    "from gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "            filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db8dd5-5874-49f8-8287-90ebc1afd291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
