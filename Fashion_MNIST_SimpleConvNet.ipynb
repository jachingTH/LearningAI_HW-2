{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d9ff294-7d53-4404-bf73-ba0e5a1e4a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2998824164408296\n",
      "=== epoch:1, train acc:0.161, test acc:0.17 ===\n",
      "train loss:2.2972735397202744\n",
      "train loss:2.2927246653877247\n",
      "train loss:2.2867421133643004\n",
      "train loss:2.2775476137351838\n",
      "train loss:2.264273186110535\n",
      "train loss:2.2456527377402384\n",
      "train loss:2.2210933361737375\n",
      "train loss:2.1959794740373635\n",
      "train loss:2.1743260905626687\n",
      "train loss:2.1737454540699312\n",
      "train loss:2.1006806411638697\n",
      "train loss:2.0907549516332233\n",
      "train loss:2.045575598978337\n",
      "train loss:1.960453901880843\n",
      "train loss:1.9115692559699324\n",
      "train loss:1.8190686829964113\n",
      "train loss:1.7962069647992123\n",
      "train loss:1.712252257138026\n",
      "train loss:1.6275435855168154\n",
      "train loss:1.5138798865804293\n",
      "train loss:1.487378774843124\n",
      "train loss:1.42593574975469\n",
      "train loss:1.262936550324087\n",
      "train loss:1.2510008671689279\n",
      "train loss:1.1634410194914824\n",
      "train loss:1.0808550158204249\n",
      "train loss:1.0451678933696995\n",
      "train loss:0.8451706956795051\n",
      "train loss:0.9297451960895757\n",
      "train loss:0.8393319553274949\n",
      "train loss:0.8639521987740965\n",
      "train loss:0.7037390656327673\n",
      "train loss:0.6813226601425731\n",
      "train loss:0.6658816351007889\n",
      "train loss:0.7774005308771597\n",
      "train loss:0.6992646397204118\n",
      "train loss:0.849665024301768\n",
      "train loss:0.5179632258709689\n",
      "train loss:0.8446558752412398\n",
      "train loss:0.6552409441934013\n",
      "train loss:0.6127879680957933\n",
      "train loss:0.7512762032066562\n",
      "train loss:0.6531503026789568\n",
      "train loss:0.5949852781585244\n",
      "train loss:0.6144222081585807\n",
      "train loss:0.6717535386823137\n",
      "train loss:0.5349211147141228\n",
      "train loss:0.3816212817817635\n",
      "train loss:0.340115583641253\n",
      "train loss:0.5502643835066704\n",
      "=== epoch:2, train acc:0.808, test acc:0.782 ===\n",
      "train loss:0.4991879131737932\n",
      "train loss:0.6386664041154717\n",
      "train loss:0.5302064243361223\n",
      "train loss:0.6814894924478411\n",
      "train loss:0.5677145678233407\n",
      "train loss:0.49096024839050856\n",
      "train loss:0.5384894287403739\n",
      "train loss:0.3203414721031777\n",
      "train loss:0.5454625532497989\n",
      "train loss:0.5471920820296525\n",
      "train loss:0.4504336127118982\n",
      "train loss:0.36085703293599075\n",
      "train loss:0.3066050456530081\n",
      "train loss:0.3382128281902693\n",
      "train loss:0.39164479781266587\n",
      "train loss:0.3777528535797833\n",
      "train loss:0.5417573807210921\n",
      "train loss:0.4346260413776015\n",
      "train loss:0.3724841765973423\n",
      "train loss:0.4083628212861935\n",
      "train loss:0.2931633229827209\n",
      "train loss:0.48363650984958767\n",
      "train loss:0.3400129674263293\n",
      "train loss:0.4944861164063707\n",
      "train loss:0.318854075549713\n",
      "train loss:0.41745859298146015\n",
      "train loss:0.5353104732137801\n",
      "train loss:0.4260848669838566\n",
      "train loss:0.4316961859728968\n",
      "train loss:0.42731081031480533\n",
      "train loss:0.25298326785477543\n",
      "train loss:0.420106336494084\n",
      "train loss:0.33776167513542604\n",
      "train loss:0.3123763426486207\n",
      "train loss:0.35266222369461725\n",
      "train loss:0.4215105908595146\n",
      "train loss:0.3552442733133411\n",
      "train loss:0.3081832699881424\n",
      "train loss:0.29160504271787036\n",
      "train loss:0.32138753914806534\n",
      "train loss:0.4392716348061135\n",
      "train loss:0.3977403465430914\n",
      "train loss:0.3418002972126988\n",
      "train loss:0.30140997465962166\n",
      "train loss:0.4920606676866909\n",
      "train loss:0.3352562465321296\n",
      "train loss:0.5398605913947204\n",
      "train loss:0.4794724577582005\n",
      "train loss:0.27543865864900335\n",
      "train loss:0.5859030041847141\n",
      "=== epoch:3, train acc:0.87, test acc:0.868 ===\n",
      "train loss:0.17262207615548436\n",
      "train loss:0.31112749463159856\n",
      "train loss:0.31083599666708994\n",
      "train loss:0.23369450145942783\n",
      "train loss:0.3124598847339542\n",
      "train loss:0.4991489504895615\n",
      "train loss:0.32616447331417975\n",
      "train loss:0.43982876418525835\n",
      "train loss:0.3464466174928852\n",
      "train loss:0.34144535663068437\n",
      "train loss:0.4794262112179117\n",
      "train loss:0.3002458006659422\n",
      "train loss:0.30987603127376834\n",
      "train loss:0.3922961547887887\n",
      "train loss:0.27042721988569407\n",
      "train loss:0.2771039757275526\n",
      "train loss:0.4158963171411893\n",
      "train loss:0.32898143424533116\n",
      "train loss:0.3226986686486108\n",
      "train loss:0.44608274009770726\n",
      "train loss:0.45066502774010375\n",
      "train loss:0.3375159423837437\n",
      "train loss:0.25793749793197507\n",
      "train loss:0.29296671832371696\n",
      "train loss:0.23734218958822628\n",
      "train loss:0.3328570718639692\n",
      "train loss:0.4047117043558013\n",
      "train loss:0.21820446272304803\n",
      "train loss:0.24192356654188507\n",
      "train loss:0.34176907537858997\n",
      "train loss:0.24515662157824475\n",
      "train loss:0.2509543855213928\n",
      "train loss:0.22621292507280227\n",
      "train loss:0.21942771077035764\n",
      "train loss:0.21292979356080177\n",
      "train loss:0.25762312580986224\n",
      "train loss:0.24954592191771177\n",
      "train loss:0.22238482995009393\n",
      "train loss:0.2928646763393593\n",
      "train loss:0.24535610165110597\n",
      "train loss:0.45039790203494867\n",
      "train loss:0.3398202737905548\n",
      "train loss:0.23119287312518114\n",
      "train loss:0.3494295949860505\n",
      "train loss:0.368600566593079\n",
      "train loss:0.21495043402072522\n",
      "train loss:0.48611579544893374\n",
      "train loss:0.30841928483589903\n",
      "train loss:0.22421665271247398\n",
      "train loss:0.17372537798549587\n",
      "=== epoch:4, train acc:0.906, test acc:0.89 ===\n",
      "train loss:0.25893365159124426\n",
      "train loss:0.2613066766329869\n",
      "train loss:0.2551737002042134\n",
      "train loss:0.3005452266133817\n",
      "train loss:0.33900744357379603\n",
      "train loss:0.14571298751872655\n",
      "train loss:0.2464359830896096\n",
      "train loss:0.19771611193205516\n",
      "train loss:0.27669829680285374\n",
      "train loss:0.2946876813811402\n",
      "train loss:0.2485631789246762\n",
      "train loss:0.3880971911630964\n",
      "train loss:0.34647526304169923\n",
      "train loss:0.34910041672592806\n",
      "train loss:0.2522571986117194\n",
      "train loss:0.23052382049954\n",
      "train loss:0.23491592217232798\n",
      "train loss:0.21684035783885514\n",
      "train loss:0.22963166681616573\n",
      "train loss:0.3107867644921468\n",
      "train loss:0.31174461283491867\n",
      "train loss:0.3005252654669516\n",
      "train loss:0.1876229289739183\n",
      "train loss:0.26179515055837116\n",
      "train loss:0.22594561253550693\n",
      "train loss:0.2764278097388224\n",
      "train loss:0.2807991806389053\n",
      "train loss:0.25905580126918404\n",
      "train loss:0.2902571292290224\n",
      "train loss:0.16887658068394365\n",
      "train loss:0.2086905595148761\n",
      "train loss:0.3848659875369676\n",
      "train loss:0.2671049302600536\n",
      "train loss:0.27628865066202396\n",
      "train loss:0.2712935837358209\n",
      "train loss:0.36372657321640306\n",
      "train loss:0.18396036325355644\n",
      "train loss:0.14851139617744416\n",
      "train loss:0.19596648460671087\n",
      "train loss:0.2594224972410941\n",
      "train loss:0.2478230889181125\n",
      "train loss:0.3533586820549586\n",
      "train loss:0.16349176974903798\n",
      "train loss:0.2778675092842351\n",
      "train loss:0.16111941305724198\n",
      "train loss:0.2643489048445127\n",
      "train loss:0.2591662008736244\n",
      "train loss:0.2358830936746909\n",
      "train loss:0.3446431838147444\n",
      "train loss:0.30943013084216436\n",
      "=== epoch:5, train acc:0.908, test acc:0.887 ===\n",
      "train loss:0.287082680595274\n",
      "train loss:0.19834547951834658\n",
      "train loss:0.20054208794929917\n",
      "train loss:0.11987699795884327\n",
      "train loss:0.35189268655677686\n",
      "train loss:0.19900828475378024\n",
      "train loss:0.1350115758483094\n",
      "train loss:0.20227509018753423\n",
      "train loss:0.22217187882013678\n",
      "train loss:0.18484260643419806\n",
      "train loss:0.19780919740602404\n",
      "train loss:0.18756585290394542\n",
      "train loss:0.34000144443863933\n",
      "train loss:0.18166184053311643\n",
      "train loss:0.19259715068589275\n",
      "train loss:0.19454074436616808\n",
      "train loss:0.19356233449352853\n",
      "train loss:0.179307650199334\n",
      "train loss:0.18664245208272912\n",
      "train loss:0.2040167686969454\n",
      "train loss:0.2987118168527271\n",
      "train loss:0.22838205013416665\n",
      "train loss:0.22880872782348735\n",
      "train loss:0.18857697672554646\n",
      "train loss:0.39957863869349447\n",
      "train loss:0.23680829123542874\n",
      "train loss:0.16336990193615364\n",
      "train loss:0.2813157590680554\n",
      "train loss:0.2397121767752462\n",
      "train loss:0.2519546958106528\n",
      "train loss:0.26504928829689356\n",
      "train loss:0.17285488724437617\n",
      "train loss:0.19572746909518135\n",
      "train loss:0.30740527354189995\n",
      "train loss:0.14351851632765075\n",
      "train loss:0.21354991196592138\n",
      "train loss:0.19343601959808485\n",
      "train loss:0.10415277142754378\n",
      "train loss:0.14648864034198333\n",
      "train loss:0.23809896331073419\n",
      "train loss:0.2974172881946491\n",
      "train loss:0.2067235429671621\n",
      "train loss:0.31227916113402043\n",
      "train loss:0.2041656738908692\n",
      "train loss:0.3107998996376594\n",
      "train loss:0.19862943497387986\n",
      "train loss:0.15171319369108627\n",
      "train loss:0.20637579830496922\n",
      "train loss:0.18330331737557085\n",
      "train loss:0.152893745065234\n",
      "=== epoch:6, train acc:0.932, test acc:0.916 ===\n",
      "train loss:0.1723905618674793\n",
      "train loss:0.17391543238344795\n",
      "train loss:0.2622320537176467\n",
      "train loss:0.14522076792372304\n",
      "train loss:0.36732733828411684\n",
      "train loss:0.17203035674429884\n",
      "train loss:0.1548636483676656\n",
      "train loss:0.3225383454280628\n",
      "train loss:0.1755139009062009\n",
      "train loss:0.26682302594435003\n",
      "train loss:0.29558614017921303\n",
      "train loss:0.21952436249968885\n",
      "train loss:0.20481482646684188\n",
      "train loss:0.19424113285298192\n",
      "train loss:0.2271840376045408\n",
      "train loss:0.1598296964657843\n",
      "train loss:0.1305498439311635\n",
      "train loss:0.18292734348024234\n",
      "train loss:0.17307090285505666\n",
      "train loss:0.19084502028389502\n",
      "train loss:0.19552330763352252\n",
      "train loss:0.1743110508111722\n",
      "train loss:0.1272951846958832\n",
      "train loss:0.2987513144680427\n",
      "train loss:0.2273057723781535\n",
      "train loss:0.28414892871534503\n",
      "train loss:0.23013142835669767\n",
      "train loss:0.1628544460733786\n",
      "train loss:0.3188198930286516\n",
      "train loss:0.20791457445267525\n",
      "train loss:0.12759450714583068\n",
      "train loss:0.2733331671052138\n",
      "train loss:0.18723613166509012\n",
      "train loss:0.17532134374176153\n",
      "train loss:0.2512057735108554\n",
      "train loss:0.2690329397098381\n",
      "train loss:0.1995451295339541\n",
      "train loss:0.125367482782537\n",
      "train loss:0.12428678043833194\n",
      "train loss:0.09320649788808638\n",
      "train loss:0.15078633649237755\n",
      "train loss:0.1992917777185479\n",
      "train loss:0.18997067151977073\n",
      "train loss:0.12468289685574066\n",
      "train loss:0.1252651481785055\n",
      "train loss:0.13613930869318897\n",
      "train loss:0.153624836042992\n",
      "train loss:0.14234635816390395\n",
      "train loss:0.10736438072026638\n",
      "train loss:0.17238695505328905\n",
      "=== epoch:7, train acc:0.936, test acc:0.918 ===\n",
      "train loss:0.15897569187405144\n",
      "train loss:0.18075519384749147\n",
      "train loss:0.12069224310943719\n",
      "train loss:0.22475381780492665\n",
      "train loss:0.13551781045037567\n",
      "train loss:0.15346473929875606\n",
      "train loss:0.16620205679342984\n",
      "train loss:0.19932981956098345\n",
      "train loss:0.11783622744403856\n",
      "train loss:0.20692907086642862\n",
      "train loss:0.2509167822589344\n",
      "train loss:0.07784624171046266\n",
      "train loss:0.12748133825316738\n",
      "train loss:0.0798111901347998\n",
      "train loss:0.0760649850539881\n",
      "train loss:0.10711259896000136\n",
      "train loss:0.21402230978962902\n",
      "train loss:0.09238421353501292\n",
      "train loss:0.07884691993866169\n",
      "train loss:0.15397301537578267\n",
      "train loss:0.19296168954785803\n",
      "train loss:0.12519133547125033\n",
      "train loss:0.18247746455104896\n",
      "train loss:0.14271540371710403\n",
      "train loss:0.11677405229803624\n",
      "train loss:0.0669551363412741\n",
      "train loss:0.15736633703086522\n",
      "train loss:0.0955066819161285\n",
      "train loss:0.15594073130858843\n",
      "train loss:0.16808387289461632\n",
      "train loss:0.1123818528616411\n",
      "train loss:0.1549792850763739\n",
      "train loss:0.11707811200355231\n",
      "train loss:0.0994878590819015\n",
      "train loss:0.21335919440375317\n",
      "train loss:0.17571128914563558\n",
      "train loss:0.18481106598665534\n",
      "train loss:0.2752909025974859\n",
      "train loss:0.15813434471637883\n",
      "train loss:0.08268875390707142\n",
      "train loss:0.0900266594298156\n",
      "train loss:0.04885396220763747\n",
      "train loss:0.10194427327997375\n",
      "train loss:0.13268030398597283\n",
      "train loss:0.20113123423996818\n",
      "train loss:0.10427622916318795\n",
      "train loss:0.14402848458496073\n",
      "train loss:0.10201892867753777\n",
      "train loss:0.11696031199268514\n",
      "train loss:0.10358409088268888\n",
      "=== epoch:8, train acc:0.953, test acc:0.932 ===\n",
      "train loss:0.10745364184853912\n",
      "train loss:0.0595596585658826\n",
      "train loss:0.13136577372338074\n",
      "train loss:0.09925060316951796\n",
      "train loss:0.20315294379052112\n",
      "train loss:0.19963285849367915\n",
      "train loss:0.18563791733030788\n",
      "train loss:0.18345908806700592\n",
      "train loss:0.08186924900677893\n",
      "train loss:0.23016969162162954\n",
      "train loss:0.11423943301724007\n",
      "train loss:0.08096736526003971\n",
      "train loss:0.18306614257667203\n",
      "train loss:0.15434380323411806\n",
      "train loss:0.1309524275987\n",
      "train loss:0.1180620179545521\n",
      "train loss:0.12196385696201684\n",
      "train loss:0.24222396175516214\n",
      "train loss:0.19425640228156538\n",
      "train loss:0.2561517566324425\n",
      "train loss:0.15087319676781352\n",
      "train loss:0.08888381830513412\n",
      "train loss:0.1393958058865584\n",
      "train loss:0.09396250247948082\n",
      "train loss:0.08822384369539422\n",
      "train loss:0.1780529394458972\n",
      "train loss:0.15371817395641976\n",
      "train loss:0.17057024893376943\n",
      "train loss:0.09242279762894862\n",
      "train loss:0.29577672893289153\n",
      "train loss:0.08700272636676715\n",
      "train loss:0.14254142943968326\n",
      "train loss:0.09781124143964785\n",
      "train loss:0.07254992032303194\n",
      "train loss:0.14855566748627505\n",
      "train loss:0.07946662119640376\n",
      "train loss:0.17310011460332017\n",
      "train loss:0.12290841346689507\n",
      "train loss:0.06231692576061221\n",
      "train loss:0.1390509885629632\n",
      "train loss:0.10238244506974273\n",
      "train loss:0.04051527523801246\n",
      "train loss:0.06974300924706\n",
      "train loss:0.12810296457089893\n",
      "train loss:0.15421496256309614\n",
      "train loss:0.08897610131511917\n",
      "train loss:0.06369484868943669\n",
      "train loss:0.14662770203471123\n",
      "train loss:0.12347600788111665\n",
      "train loss:0.14011764316516054\n",
      "=== epoch:9, train acc:0.957, test acc:0.941 ===\n",
      "train loss:0.17332627835393388\n",
      "train loss:0.16529140599080944\n",
      "train loss:0.0678363577354947\n",
      "train loss:0.12504762486359597\n",
      "train loss:0.1669047944152527\n",
      "train loss:0.14400180923556646\n",
      "train loss:0.14590620426363615\n",
      "train loss:0.11401609472581108\n",
      "train loss:0.07273780127065216\n",
      "train loss:0.1249798332314817\n",
      "train loss:0.0917894480586709\n",
      "train loss:0.09029432598807498\n",
      "train loss:0.17083997525360575\n",
      "train loss:0.07034297858544135\n",
      "train loss:0.10501782809882827\n",
      "train loss:0.09043008914402487\n",
      "train loss:0.039330413061402586\n",
      "train loss:0.10684975162707486\n",
      "train loss:0.21603724294466667\n",
      "train loss:0.07650029288024221\n",
      "train loss:0.0977222998910932\n",
      "train loss:0.08906739472311954\n",
      "train loss:0.11513549005782313\n",
      "train loss:0.12886814906085353\n",
      "train loss:0.1107325203716527\n",
      "train loss:0.13496781906856145\n",
      "train loss:0.09664061553284427\n",
      "train loss:0.07884874279237125\n",
      "train loss:0.12200212460390192\n",
      "train loss:0.17933416955710402\n",
      "train loss:0.17623150098482518\n",
      "train loss:0.06599447202902763\n",
      "train loss:0.11187699198395103\n",
      "train loss:0.09346421361659991\n",
      "train loss:0.1321402834214865\n",
      "train loss:0.08528703402460953\n",
      "train loss:0.11451163366157394\n",
      "train loss:0.06350115470223029\n",
      "train loss:0.11674163606320694\n",
      "train loss:0.13568918752370782\n",
      "train loss:0.06753522272875415\n",
      "train loss:0.08336585061803756\n",
      "train loss:0.09837393397744718\n",
      "train loss:0.07341345700416002\n",
      "train loss:0.08729419696551982\n",
      "train loss:0.08370995941441949\n",
      "train loss:0.1199994343688521\n",
      "train loss:0.05922746235892614\n",
      "train loss:0.17890503817504189\n",
      "train loss:0.10387741526634432\n",
      "=== epoch:10, train acc:0.963, test acc:0.943 ===\n",
      "train loss:0.06874239733956458\n",
      "train loss:0.08271855601098052\n",
      "train loss:0.1111287568516623\n",
      "train loss:0.10121807821699319\n",
      "train loss:0.1054761533522899\n",
      "train loss:0.10062849685797955\n",
      "train loss:0.10395772936670099\n",
      "train loss:0.06138238190645524\n",
      "train loss:0.08290799395301425\n",
      "train loss:0.049862956110179414\n",
      "train loss:0.169670998237716\n",
      "train loss:0.09272258095540639\n",
      "train loss:0.05449560398244628\n",
      "train loss:0.10721129715927226\n",
      "train loss:0.03711692982203141\n",
      "train loss:0.10027145063439673\n",
      "train loss:0.12419280780492287\n",
      "train loss:0.06281606712236412\n",
      "train loss:0.05595649882249179\n",
      "train loss:0.09591454577080308\n",
      "train loss:0.055379001842804476\n",
      "train loss:0.04519410504564713\n",
      "train loss:0.08432847902474679\n",
      "train loss:0.16183035008313662\n",
      "train loss:0.056144303020053125\n",
      "train loss:0.12964882230876615\n",
      "train loss:0.080069340680984\n",
      "train loss:0.14091694906426627\n",
      "train loss:0.1356870051105235\n",
      "train loss:0.052391707834085234\n",
      "train loss:0.1210189618161929\n",
      "train loss:0.0708406757398836\n",
      "train loss:0.07762198434743384\n",
      "train loss:0.057608515883941766\n",
      "train loss:0.07473681772054173\n",
      "train loss:0.05022664739248912\n",
      "train loss:0.02610885033580247\n",
      "train loss:0.07171974018113214\n",
      "train loss:0.07598852672652201\n",
      "train loss:0.06030600778076559\n",
      "train loss:0.03493764751681384\n",
      "train loss:0.1012965537200229\n",
      "train loss:0.0700057734738999\n",
      "train loss:0.0598691924281604\n",
      "train loss:0.15302785875835861\n",
      "train loss:0.04347301233187746\n",
      "train loss:0.11166760794950709\n",
      "train loss:0.11197704811010344\n",
      "train loss:0.05239171972811277\n",
      "train loss:0.05880172616253569\n",
      "=== epoch:11, train acc:0.961, test acc:0.945 ===\n",
      "train loss:0.059800103020062466\n",
      "train loss:0.064686224618466\n",
      "train loss:0.08665821034611584\n",
      "train loss:0.029224598513412685\n",
      "train loss:0.11086704358324262\n",
      "train loss:0.06107319896333137\n",
      "train loss:0.08601855452072166\n",
      "train loss:0.10925840088332475\n",
      "train loss:0.044929450084253225\n",
      "train loss:0.017622408765892827\n",
      "train loss:0.08121928006375599\n",
      "train loss:0.17978881053084378\n",
      "train loss:0.15018370121234656\n",
      "train loss:0.07382769752635182\n",
      "train loss:0.0622388911328356\n",
      "train loss:0.10062108803229215\n",
      "train loss:0.06495405477949319\n",
      "train loss:0.0862428316465327\n",
      "train loss:0.05624256478452435\n",
      "train loss:0.11383980277593475\n",
      "train loss:0.12190460767130594\n",
      "train loss:0.1851406558849781\n",
      "train loss:0.03670514004040581\n",
      "train loss:0.09952448751993061\n",
      "train loss:0.11810724875084437\n",
      "train loss:0.12149360219754943\n",
      "train loss:0.0358026795927639\n",
      "train loss:0.09672598601418607\n",
      "train loss:0.0634057752214342\n",
      "train loss:0.11467277898599951\n",
      "train loss:0.07726594784940721\n",
      "train loss:0.037825478256917594\n",
      "train loss:0.1397532552856147\n",
      "train loss:0.0382965000096806\n",
      "train loss:0.08352244070810196\n",
      "train loss:0.13687030368072128\n",
      "train loss:0.09634737236705922\n",
      "train loss:0.09382813213561185\n",
      "train loss:0.04966303128052977\n",
      "train loss:0.11155230666847923\n",
      "train loss:0.0699205849261734\n",
      "train loss:0.05576237320912757\n",
      "train loss:0.15872835798131868\n",
      "train loss:0.0395944908235717\n",
      "train loss:0.14057243846179499\n",
      "train loss:0.0554713182065961\n",
      "train loss:0.07378343305667445\n",
      "train loss:0.07043193634794825\n",
      "train loss:0.04824852257375855\n",
      "train loss:0.07779525562188244\n",
      "=== epoch:12, train acc:0.972, test acc:0.951 ===\n",
      "train loss:0.03049348342195153\n",
      "train loss:0.08505427155735937\n",
      "train loss:0.12349644486984851\n",
      "train loss:0.10239140880557691\n",
      "train loss:0.07940429354074513\n",
      "train loss:0.11066432751076034\n",
      "train loss:0.057340265834276545\n",
      "train loss:0.044582786834353995\n",
      "train loss:0.06125493010292666\n",
      "train loss:0.032805753255956416\n",
      "train loss:0.023341841187046245\n",
      "train loss:0.035286465748126966\n",
      "train loss:0.030367781294064162\n",
      "train loss:0.06431987980355502\n",
      "train loss:0.04368495493793977\n",
      "train loss:0.11429334468472932\n",
      "train loss:0.06207531980325118\n",
      "train loss:0.02978805442638631\n",
      "train loss:0.04336248234398507\n",
      "train loss:0.025085937632962737\n",
      "train loss:0.11219364680433648\n",
      "train loss:0.08173162568915864\n",
      "train loss:0.06652159426396921\n",
      "train loss:0.031701938781039524\n",
      "train loss:0.07726987911370577\n",
      "train loss:0.05187814970432083\n",
      "train loss:0.048588565735111285\n",
      "train loss:0.027699189587235017\n",
      "train loss:0.0837422886941216\n",
      "train loss:0.05820391091949334\n",
      "train loss:0.13087719861068914\n",
      "train loss:0.06683585202434202\n",
      "train loss:0.13042208148223375\n",
      "train loss:0.03149986516428454\n",
      "train loss:0.025658544213014344\n",
      "train loss:0.15421060885796087\n",
      "train loss:0.04863374758781168\n",
      "train loss:0.08713872475483615\n",
      "train loss:0.02687794709374908\n",
      "train loss:0.12981999439544262\n",
      "train loss:0.056568895438203357\n",
      "train loss:0.07706151005492166\n",
      "train loss:0.034832975099851164\n",
      "train loss:0.03613193888498191\n",
      "train loss:0.0721547897586566\n",
      "train loss:0.08046564475384231\n",
      "train loss:0.08724850995491834\n",
      "train loss:0.05083613194118168\n",
      "train loss:0.10999302072042712\n",
      "train loss:0.050195717816537026\n",
      "=== epoch:13, train acc:0.973, test acc:0.948 ===\n",
      "train loss:0.04843586701364821\n",
      "train loss:0.05679807830178043\n",
      "train loss:0.05404354837675071\n",
      "train loss:0.11023422140114443\n",
      "train loss:0.054042837226406674\n",
      "train loss:0.03850235793871708\n",
      "train loss:0.06316690273701528\n",
      "train loss:0.05934305471375888\n",
      "train loss:0.056713516921798816\n",
      "train loss:0.06860263127692103\n",
      "train loss:0.07819554891348549\n",
      "train loss:0.03746583909156611\n",
      "train loss:0.04029207344391345\n",
      "train loss:0.025005790292009385\n",
      "train loss:0.027062771220561\n",
      "train loss:0.024752411380446947\n",
      "train loss:0.1247497696531848\n",
      "train loss:0.07085666470439858\n",
      "train loss:0.04506364596521162\n",
      "train loss:0.02176160489495045\n",
      "train loss:0.06089603327176965\n",
      "train loss:0.05645496782790646\n",
      "train loss:0.022150980761955368\n",
      "train loss:0.0349911513722764\n",
      "train loss:0.017262822150968752\n",
      "train loss:0.024330035346461575\n",
      "train loss:0.08014391718663721\n",
      "train loss:0.09736525562011869\n",
      "train loss:0.021320938305402937\n",
      "train loss:0.07551445078814105\n",
      "train loss:0.06929823346007882\n",
      "train loss:0.03538864229490525\n",
      "train loss:0.05786345564312237\n",
      "train loss:0.051253041162914\n",
      "train loss:0.08837925165796513\n",
      "train loss:0.08677842877645184\n",
      "train loss:0.11200294994494023\n",
      "train loss:0.09620169298370666\n",
      "train loss:0.042392095753069486\n",
      "train loss:0.10716617205558496\n",
      "train loss:0.046019440483048894\n",
      "train loss:0.04778373144091098\n",
      "train loss:0.054115418808296886\n",
      "train loss:0.028598232895625623\n",
      "train loss:0.061378146093113026\n",
      "train loss:0.09427158462924284\n",
      "train loss:0.05532727716223834\n",
      "train loss:0.05281627952459312\n",
      "train loss:0.024643347591592785\n",
      "train loss:0.027311658405256258\n",
      "=== epoch:14, train acc:0.978, test acc:0.946 ===\n",
      "train loss:0.018957924543214703\n",
      "train loss:0.05238378142414388\n",
      "train loss:0.0711608166801641\n",
      "train loss:0.047948484450526616\n",
      "train loss:0.11634371414304934\n",
      "train loss:0.045311032153214185\n",
      "train loss:0.010761738368951715\n",
      "train loss:0.030574836989237575\n",
      "train loss:0.06229557055481831\n",
      "train loss:0.05665264790030583\n",
      "train loss:0.08572822400342703\n",
      "train loss:0.05230061164154618\n",
      "train loss:0.11632613837233417\n",
      "train loss:0.033943476843509006\n",
      "train loss:0.04883333365148238\n",
      "train loss:0.07765833952065344\n",
      "train loss:0.03828829598485134\n",
      "train loss:0.04059847745105945\n",
      "train loss:0.009515067040597203\n",
      "train loss:0.0537595638635759\n",
      "train loss:0.040217655881008\n",
      "train loss:0.0623603495485571\n",
      "train loss:0.06420840650165725\n",
      "train loss:0.04190384889481222\n",
      "train loss:0.05117617564263944\n",
      "train loss:0.04763022915397788\n",
      "train loss:0.02937704886823303\n",
      "train loss:0.02126941410447368\n",
      "train loss:0.03130558797082995\n",
      "train loss:0.04306311321605761\n",
      "train loss:0.044101632329135956\n",
      "train loss:0.045394492488605095\n",
      "train loss:0.044089639352098205\n",
      "train loss:0.05098072660265575\n",
      "train loss:0.06351387767695515\n",
      "train loss:0.03328364363373307\n",
      "train loss:0.09759243101610851\n",
      "train loss:0.09197975490929026\n",
      "train loss:0.022990288080553284\n",
      "train loss:0.09723259476975747\n",
      "train loss:0.03499415937086565\n",
      "train loss:0.036985907418833175\n",
      "train loss:0.05816078580524944\n",
      "train loss:0.028791568868100238\n",
      "train loss:0.0191765087025821\n",
      "train loss:0.03719600348322698\n",
      "train loss:0.03260333240440588\n",
      "train loss:0.036774045316545136\n",
      "train loss:0.020497704536450022\n",
      "train loss:0.02322502510780357\n",
      "=== epoch:15, train acc:0.981, test acc:0.955 ===\n",
      "train loss:0.05381586573304783\n",
      "train loss:0.010960383002430865\n",
      "train loss:0.05197066192958049\n",
      "train loss:0.03831585623928475\n",
      "train loss:0.02180266587203109\n",
      "train loss:0.02210986316990079\n",
      "train loss:0.028359006459942947\n",
      "train loss:0.01618304351465715\n",
      "train loss:0.027058519222064446\n",
      "train loss:0.039958494347460696\n",
      "train loss:0.04627259965752312\n",
      "train loss:0.03109807206166001\n",
      "train loss:0.025182346453877725\n",
      "train loss:0.07052302030795084\n",
      "train loss:0.0649276134186714\n",
      "train loss:0.014720011136014795\n",
      "train loss:0.02392510536948424\n",
      "train loss:0.07784792515745005\n",
      "train loss:0.05700619105281934\n",
      "train loss:0.021056648017496217\n",
      "train loss:0.06401161253774386\n",
      "train loss:0.0401405858894233\n",
      "train loss:0.06610443599420075\n",
      "train loss:0.017690735697730997\n",
      "train loss:0.025890200160151616\n",
      "train loss:0.0627226232174401\n",
      "train loss:0.05218304002063963\n",
      "train loss:0.027024493812238957\n",
      "train loss:0.054483105166870785\n",
      "train loss:0.015180289752462946\n",
      "train loss:0.051026079000355405\n",
      "train loss:0.02098177508646594\n",
      "train loss:0.008480013482335503\n",
      "train loss:0.019734803748276245\n",
      "train loss:0.02187683369018372\n",
      "train loss:0.03162379773407732\n",
      "train loss:0.0452451756202791\n",
      "train loss:0.015392884138699882\n",
      "train loss:0.08258597298610741\n",
      "train loss:0.025728581337921903\n",
      "train loss:0.019020950829943546\n",
      "train loss:0.05268540023947102\n",
      "train loss:0.02564937216883675\n",
      "train loss:0.008927300725142287\n",
      "train loss:0.06973416658765785\n",
      "train loss:0.02251542231868082\n",
      "train loss:0.00897816520711596\n",
      "train loss:0.044842086630325365\n",
      "train loss:0.03260544789930703\n",
      "train loss:0.05952300977795771\n",
      "=== epoch:16, train acc:0.985, test acc:0.961 ===\n",
      "train loss:0.04656801266111704\n",
      "train loss:0.022904030349174112\n",
      "train loss:0.035203117386585776\n",
      "train loss:0.028733731066983844\n",
      "train loss:0.0461627733711678\n",
      "train loss:0.02270278764097485\n",
      "train loss:0.01817886467120532\n",
      "train loss:0.02219879077851452\n",
      "train loss:0.02977388369149515\n",
      "train loss:0.016967960089528614\n",
      "train loss:0.032758001347210894\n",
      "train loss:0.03732122179232541\n",
      "train loss:0.020011132314195285\n",
      "train loss:0.051375450203638234\n",
      "train loss:0.041834920461433264\n",
      "train loss:0.049566270832089664\n",
      "train loss:0.040604868471202996\n",
      "train loss:0.0180337613910839\n",
      "train loss:0.02190493057224888\n",
      "train loss:0.03155228151868126\n",
      "train loss:0.015107631829572357\n",
      "train loss:0.05759622299373861\n",
      "train loss:0.009432823349210919\n",
      "train loss:0.019110157374597914\n",
      "train loss:0.021988850868362993\n",
      "train loss:0.01778058259184387\n",
      "train loss:0.017369673223020944\n",
      "train loss:0.013731506369407074\n",
      "train loss:0.019033473727257957\n",
      "train loss:0.06663010697006251\n",
      "train loss:0.02557373900268586\n",
      "train loss:0.031220714596491127\n",
      "train loss:0.058914870427272954\n",
      "train loss:0.04442396713304682\n",
      "train loss:0.01920935586213731\n",
      "train loss:0.03568771064228357\n",
      "train loss:0.027315953009082978\n",
      "train loss:0.01380103367980873\n",
      "train loss:0.025591252623291507\n",
      "train loss:0.03281511792159228\n",
      "train loss:0.018385173574821848\n",
      "train loss:0.030002768732571634\n",
      "train loss:0.01881953953162231\n",
      "train loss:0.059355033461592585\n",
      "train loss:0.024050489884927098\n",
      "train loss:0.03977183940309037\n",
      "train loss:0.033084989582376094\n",
      "train loss:0.030698643814605995\n",
      "train loss:0.03837774740115993\n",
      "train loss:0.025357378991728456\n",
      "=== epoch:17, train acc:0.991, test acc:0.959 ===\n",
      "train loss:0.023225769791645385\n",
      "train loss:0.034933915391645386\n",
      "train loss:0.02248473433881778\n",
      "train loss:0.026213484115019496\n",
      "train loss:0.019233100559753292\n",
      "train loss:0.06360124406151324\n",
      "train loss:0.04245826328614099\n",
      "train loss:0.026765913677089408\n",
      "train loss:0.021631352033703657\n",
      "train loss:0.01660288028508646\n",
      "train loss:0.016967732620846533\n",
      "train loss:0.01523804451571273\n",
      "train loss:0.010791438876863235\n",
      "train loss:0.0068050655517747525\n",
      "train loss:0.026700823323288346\n",
      "train loss:0.03054098874211566\n",
      "train loss:0.02553535936097319\n",
      "train loss:0.04230528509813244\n",
      "train loss:0.07449271892205446\n",
      "train loss:0.020783171685350564\n",
      "train loss:0.01438923640781488\n",
      "train loss:0.10852313230405172\n",
      "train loss:0.016600639773663495\n",
      "train loss:0.027693722473440638\n",
      "train loss:0.040375349077841675\n",
      "train loss:0.019337446177818865\n",
      "train loss:0.05041405769531249\n",
      "train loss:0.050427127528385594\n",
      "train loss:0.027862199532585775\n",
      "train loss:0.04600642879437124\n",
      "train loss:0.015649559068208724\n",
      "train loss:0.012764541740446689\n",
      "train loss:0.031755765322288985\n",
      "train loss:0.052490781601806875\n",
      "train loss:0.00891097802280536\n",
      "train loss:0.009997987573291272\n",
      "train loss:0.05457469440625173\n",
      "train loss:0.033838503244905414\n",
      "train loss:0.02422582498355619\n",
      "train loss:0.026115911807082823\n",
      "train loss:0.03710849300275675\n",
      "train loss:0.015678108429360134\n",
      "train loss:0.009514205536770513\n",
      "train loss:0.03559287139767713\n",
      "train loss:0.023880354483892897\n",
      "train loss:0.04154177863246712\n",
      "train loss:0.040107050949234724\n",
      "train loss:0.01717436555204836\n",
      "train loss:0.015214060527508306\n",
      "train loss:0.010956209150280546\n",
      "=== epoch:18, train acc:0.991, test acc:0.959 ===\n",
      "train loss:0.026325767554084445\n",
      "train loss:0.025366182631735974\n",
      "train loss:0.04457432994018563\n",
      "train loss:0.00691767348931141\n",
      "train loss:0.008683653455033457\n",
      "train loss:0.00707275966994506\n",
      "train loss:0.009865277303210612\n",
      "train loss:0.015253669701965731\n",
      "train loss:0.030171221179019145\n",
      "train loss:0.028457331208891743\n",
      "train loss:0.02850797315077912\n",
      "train loss:0.025304280530282774\n",
      "train loss:0.027145555964374936\n",
      "train loss:0.05500117619628155\n",
      "train loss:0.031475184695948365\n",
      "train loss:0.03263063927091058\n",
      "train loss:0.03397716463022319\n",
      "train loss:0.024717159649593784\n",
      "train loss:0.041173211338602626\n",
      "train loss:0.05185320581241294\n",
      "train loss:0.037929814113582885\n",
      "train loss:0.034749901790430075\n",
      "train loss:0.019261343485111947\n",
      "train loss:0.035459393309166164\n",
      "train loss:0.014211319585677774\n",
      "train loss:0.01826694232731093\n",
      "train loss:0.03289175067853861\n",
      "train loss:0.018919307374230098\n",
      "train loss:0.03942525540320792\n",
      "train loss:0.015207426456323345\n",
      "train loss:0.012067979033217425\n",
      "train loss:0.02451185552815587\n",
      "train loss:0.01589396019935211\n",
      "train loss:0.04140908927833967\n",
      "train loss:0.013746169399386372\n",
      "train loss:0.027059811570754398\n",
      "train loss:0.03404018822149969\n",
      "train loss:0.017915407099670485\n",
      "train loss:0.01900809619433653\n",
      "train loss:0.020405401981628123\n",
      "train loss:0.02641077974589617\n",
      "train loss:0.055384546398158194\n",
      "train loss:0.048871429335600985\n",
      "train loss:0.03592026400573957\n",
      "train loss:0.031005872618756366\n",
      "train loss:0.007534944248047252\n",
      "train loss:0.029005470944487577\n",
      "train loss:0.008680167725272293\n",
      "train loss:0.018748602394413942\n",
      "train loss:0.012332011630034697\n",
      "=== epoch:19, train acc:0.994, test acc:0.958 ===\n",
      "train loss:0.008699672062979433\n",
      "train loss:0.012963516572132773\n",
      "train loss:0.014148077126819859\n",
      "train loss:0.016592928430647268\n",
      "train loss:0.014058698624481124\n",
      "train loss:0.03840173836225219\n",
      "train loss:0.041317965493157555\n",
      "train loss:0.019196330804995988\n",
      "train loss:0.013258692217449486\n",
      "train loss:0.009416741640946694\n",
      "train loss:0.03282108917469751\n",
      "train loss:0.07700088261960426\n",
      "train loss:0.0245552041774809\n",
      "train loss:0.01039276873803164\n",
      "train loss:0.040290267660152784\n",
      "train loss:0.016922550695716618\n",
      "train loss:0.015376623596375922\n",
      "train loss:0.02032700432339276\n",
      "train loss:0.010472617463009344\n",
      "train loss:0.013048499453462075\n",
      "train loss:0.007183511973762442\n",
      "train loss:0.01753667445152391\n",
      "train loss:0.03433438335811307\n",
      "train loss:0.017112334293579938\n",
      "train loss:0.01880004424299324\n",
      "train loss:0.014231030732510091\n",
      "train loss:0.009010453359811859\n",
      "train loss:0.012433775589681197\n",
      "train loss:0.03208494442040167\n",
      "train loss:0.03408864802463424\n",
      "train loss:0.013176387064225576\n",
      "train loss:0.010364230005275308\n",
      "train loss:0.011588503354434303\n",
      "train loss:0.03161921940810164\n",
      "train loss:0.005352892501607004\n",
      "train loss:0.008220546224012837\n",
      "train loss:0.013081251371451202\n",
      "train loss:0.01428251009772596\n",
      "train loss:0.01749066904349664\n",
      "train loss:0.019475333054889552\n",
      "train loss:0.007884745634415392\n",
      "train loss:0.03266854388253721\n",
      "train loss:0.017431791947890966\n",
      "train loss:0.027796503020739854\n",
      "train loss:0.02210523949455531\n",
      "train loss:0.019089111054222053\n",
      "train loss:0.0722248745766632\n",
      "train loss:0.009551501162266553\n",
      "train loss:0.025321141443772902\n",
      "train loss:0.04325734507933542\n",
      "=== epoch:20, train acc:0.993, test acc:0.957 ===\n",
      "train loss:0.048985295137606376\n",
      "train loss:0.019308068130679912\n",
      "train loss:0.008466022244529\n",
      "train loss:0.021728863027141974\n",
      "train loss:0.020012388400562752\n",
      "train loss:0.011464457455524626\n",
      "train loss:0.02428492879230421\n",
      "train loss:0.025556603321184874\n",
      "train loss:0.013572555629346934\n",
      "train loss:0.01111733283341041\n",
      "train loss:0.01516594873430856\n",
      "train loss:0.034039009113868354\n",
      "train loss:0.015335696091583768\n",
      "train loss:0.009701055997273942\n",
      "train loss:0.017751855233419225\n",
      "train loss:0.016697690259481344\n",
      "train loss:0.022265631397881423\n",
      "train loss:0.015470331406284888\n",
      "train loss:0.0036393674998007786\n",
      "train loss:0.012832153819750393\n",
      "train loss:0.011933603331917501\n",
      "train loss:0.01155958357578799\n",
      "train loss:0.010909193008090914\n",
      "train loss:0.015529231573309652\n",
      "train loss:0.005662151935065994\n",
      "train loss:0.012225192155665412\n",
      "train loss:0.007808245041476346\n",
      "train loss:0.01042691380872738\n",
      "train loss:0.010595784906724954\n",
      "train loss:0.01948079057245444\n",
      "train loss:0.015232946558584677\n",
      "train loss:0.009107226622557234\n",
      "train loss:0.022973678994534598\n",
      "train loss:0.015783942441347138\n",
      "train loss:0.006985057362227761\n",
      "train loss:0.010793403647281763\n",
      "train loss:0.009097553223102177\n",
      "train loss:0.02465997883842655\n",
      "train loss:0.01862459485448097\n",
      "train loss:0.01903305042782129\n",
      "train loss:0.03982478587871797\n",
      "train loss:0.014394372111870063\n",
      "train loss:0.005129453407524642\n",
      "train loss:0.03135483534415271\n",
      "train loss:0.0200835008105043\n",
      "train loss:0.035043987860546556\n",
      "train loss:0.009331543118213192\n",
      "train loss:0.00959133122127757\n",
      "train loss:0.014947488537224194\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.957\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from layers import *\n",
    "from gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "            filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db8dd5-5874-49f8-8287-90ebc1afd291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
